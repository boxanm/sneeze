<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="https://norlab.ulaval.ca/atom.xml" rel="self" type="application/atom+xml" /><link href="https://norlab.ulaval.ca/" rel="alternate" type="text/html" /><updated>2019-05-17T15:31:48-04:00</updated><id>https://norlab.ulaval.ca/atom.xml</id><title type="html">Northern Robotics Laboratory</title><subtitle>Website showcasing research and news from the Northern Robotics Laboratory, Laval University</subtitle><entry><title type="html">Satellite Visibility Prediction</title><link href="https://norlab.ulaval.ca/research/satellite-visibility/" rel="alternate" type="text/html" title="Satellite Visibility Prediction" /><published>2019-05-09T00:00:00-04:00</published><updated>2019-05-09T00:00:00-04:00</updated><id>https://norlab.ulaval.ca/research/satellite-visibility</id><content type="html" xml:base="https://norlab.ulaval.ca/research/satellite-visibility/">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;To help future mobile agents plan their movement in harsh environments,a predictive model has been designed to determine what areas would be favorable for Global Navigation Satellite System (GNSS) positioning. The model is able to predict the number of viable satellites for a GNSS receiver, based on a 3D point cloud map and a satellite constellation. Both occlusion and absorption effects of the environment are considered. A rugged mobile platform was designed to collect data in order to generate the point cloud maps. It was deployed during the Canadian winter known for large amounts of snow and extremely low temperatures. The test environments include a highly dense boreal forest and a university campus with high buildings. The experiment results indicate that the model performs well in both structured and unstructured environments.&lt;/p&gt;

&lt;h1 id=&quot;quick-facts&quot;&gt;Quick facts:&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;This method has been tested in diverse environments such has a highly dense boreal forest and a university campus with high buildings&lt;/li&gt;
  &lt;li&gt;This method is able to differentiate between forest and buildings&lt;/li&gt;
  &lt;li&gt;The hardware used in the paper has been proven to work in sub-arctic environment&lt;/li&gt;
  &lt;li&gt;Our preprint is available &lt;a href=&quot;https://arxiv.org/abs/1904.07837&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;video&quot;&gt;Video&lt;/h1&gt;
&lt;!-- blank line --&gt;
&lt;figure class=&quot;video_container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/5LdxV1-9rEE&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/figure&gt;
&lt;!-- blank line --&gt;

&lt;h1 id=&quot;publications&quot;&gt;Publications&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot; reversed=&quot;reversed&quot;&gt;&lt;li&gt;&lt;span id=&quot;Dandurand2019&quot;&gt;Dandurand, P., Babin, P., Kubelka, V., Giguère, P., &amp;amp; Pomerleau, F. (2019). Predicting GNSS satellite visibility from dense point clouds. In &lt;i&gt;preprint, ArXiv. Submitted to Field and Service Robotics (FSR)&lt;/i&gt;.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Dandurand2019/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Philippe Dandurand</name></author><category term="project" /><category term="GNSS" /><category term="GPS" /><category term="lidar" /><category term="RTK" /><category term="DGNSS" /><category term="winter" /><category term="mapping" /><category term="uncertainty" /><summary type="html">Abstract To help future mobile agents plan their movement in harsh environments,a predictive model has been designed to determine what areas would be favorable for Global Navigation Satellite System (GNSS) positioning. The model is able to predict the number of viable satellites for a GNSS receiver, based on a 3D point cloud map and a satellite constellation. Both occlusion and absorption effects of the environment are considered. A rugged mobile platform was designed to collect data in order to generate the point cloud maps. It was deployed during the Canadian winter known for large amounts of snow and extremely low temperatures. The test environments include a highly dense boreal forest and a university campus with high buildings. The experiment results indicate that the model performs well in both structured and unstructured environments.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22feature%22=%3E%22projects/sat_vis_feature.jpg%22,%20%22teaser%22=%3E%22projects/sat_vis_teaser.jpg%22,%20%22thumb%22=%3Enil%7D" /></entry><entry><title type="html">First appearance of the lab in IEEE Spectrum</title><link href="https://norlab.ulaval.ca/news/ieee-spectrum/" rel="alternate" type="text/html" title="First appearance of the lab in IEEE Spectrum" /><published>2019-05-07T00:00:00-04:00</published><updated>2019-05-07T00:00:00-04:00</updated><id>https://norlab.ulaval.ca/news/ieee-spectrum</id><content type="html" xml:base="https://norlab.ulaval.ca/news/ieee-spectrum/">&lt;p&gt;Norlab appears in the online version of the magazine IEEE Spectrum with the title &lt;a href=&quot;https://spectrum.ieee.org/automaton/robotics/robotics-hardware/darpa-subt-meet-the-first-nine-teams&quot;&gt;DARPA Subterranean Challenge: Meet the First 9 Teams&lt;/a&gt;.
They confuse our French and English name, but at least the URL is correct.&lt;/p&gt;

&lt;p&gt;Our lab is supporting the mapping capability of the Team CRAS (Center for Robotics and Autonomous Systems) leaded by the Czech Technical University in Prague, Czech Republic.&lt;/p&gt;</content><author><name>François Pomerleau</name><email>francois.pomerleau@ift.ulaval.ca</email></author><category term="media" /><summary type="html">Norlab appears in the online version of the magazine IEEE Spectrum with the title DARPA Subterranean Challenge: Meet the First 9 Teams. They confuse our French and English name, but at least the URL is correct.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22teaser%22=%3E%22/logos/ieeeSpectrum.jpg%22%7D" /></entry><entry><title type="html">Information for New Students</title><link href="https://norlab.ulaval.ca/research/new-students/" rel="alternate" type="text/html" title="Information for New Students" /><published>2019-05-07T00:00:00-04:00</published><updated>2019-05-07T00:00:00-04:00</updated><id>https://norlab.ulaval.ca/research/new-students</id><content type="html" xml:base="https://norlab.ulaval.ca/research/new-students/">&lt;p&gt;Welcome to the laboratory!
It will take a couple of weeks to get you up and running in the lab, so why not use that time to explore different sources of information that will save you time later.&lt;/p&gt;

&lt;h1 id=&quot;register-to-news-feeds&quot;&gt;Register to news feeds&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Register to the magazine &lt;a href=&quot;https://www.universityaffairs.ca&quot;&gt;University Affairs&lt;/a&gt; (free) for general information about the academic life.&lt;/li&gt;
  &lt;li&gt;Register to &lt;a href=&quot;https://www.eu-robotics.net/eurobotics/newsroom/mailing-list&quot;&gt;euRobotics mailing list&lt;/a&gt;  for jobs, software release, call for journal special issues, etc.&lt;/li&gt;
  &lt;li&gt;Register to &lt;a href=&quot;http://duerer.usc.edu/mailman/listinfo.cgi/robotics-worldwide&quot;&gt;robotics-worldwide mailing list&lt;/a&gt;. Same information as euRobotics with a larger audience.&lt;/li&gt;
  &lt;li&gt;Don’t take your work &lt;a href=&quot;http://phdcomics.com/&quot;&gt;too seriously&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;explore-our-github-repo&quot;&gt;Explore our GitHub repo&lt;/h1&gt;

&lt;p&gt;You will need an account on GitHub, so you can be added to the members of&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;
&lt;a href=&quot;https://github.com/norlab-ulaval&quot; target=&quot;_blank&quot; class=&quot;btn&quot;&gt;
&lt;img src=&quot;/images/logos/github.svg&quot; style=&quot;padding-right: 1em; width: 4em; -webkit-filter: invert(100%); filter: invert(100%);&quot; /&gt;
norlab-ulaval
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Then, explore the following:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/norlab-ulaval?utf8=%E2%9C%93&amp;amp;q=poster&amp;amp;type=&amp;amp;language=&quot;&gt;Scientific posters&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/norlab-ulaval?utf8=%E2%9C%93&amp;amp;q=publication&amp;amp;type=&amp;amp;language=&quot;&gt;Scientific publications&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/norlab-ulaval?utf8=%E2%9C%93&amp;amp;q=mastersProposal&amp;amp;type=&amp;amp;language=&quot;&gt;Master’s proposal&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/norlab-ulaval?utf8=%E2%9C%93&amp;amp;q=mastersThesis&amp;amp;type=&amp;amp;language=&quot;&gt;Master’s thesis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/norlab-ulaval/visualIdentity&quot;&gt;Different logos of the lab&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Read our &lt;a href=&quot;https://github.com/norlab-ulaval/latexGoodPractices/blob/master/preamble.tex&quot;&gt;Latex good practices&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;little-tip-to-save-you-some-time&quot;&gt;Little tip to save you some time&lt;/h1&gt;
&lt;p&gt;Create a new bookmark on you favorite web browser and copy-paste this script as the url:&lt;/p&gt;
&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nx&quot;&gt;javascript&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(){&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;location&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;href&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'https://acces.bibl.ulaval.ca/login?qurl='&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+%&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;encodeURIComponent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;location&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;href&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);})());&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;If you click the bookmark when you are on a scientific article publisher website (e.g. Springer), it will log you into Ariane automatically. Ariane grants you access to the article without having to pay if the university has an agreement with the publisher.&lt;/p&gt;

&lt;h1 id=&quot;be-social&quot;&gt;Be social&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Like our Facebook page &lt;a href=&quot;https://www.facebook.com/norlab.ulaval/&quot;&gt;norlab.ulaval&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Follow the LinkedIn page &lt;a href=&quot;https://www.linkedin.com/company/norlab/&quot;&gt;Norlab&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Subscribe to our YouTube channel &lt;a href=&quot;https://www.youtube.com/channel/UCh9G8xpr72lBiyWyBKXBTXA&quot;&gt;norlab&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Ask to be added on our ResearchGate &lt;a href=&quot;https://www.researchgate.net/lab/Northern-Robotics-Laboratory-Norlab-Francois-Pomerleau&quot;&gt;laboratory page&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;In general, reshare news from the lab to ensure maximum impact of our work&lt;/li&gt;
&lt;/ul&gt;</content><author><name>François Pomerleau</name><email>francois.pomerleau@ift.ulaval.ca</email></author><category term="howto" /><category term="grants" /><category term="salary" /><category term="students" /><summary type="html">Welcome to the laboratory! It will take a couple of weeks to get you up and running in the lab, so why not use that time to explore different sources of information that will save you time later.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22teaser%22=%3Enil,%20%22feature%22=%3Enil%7D" /></entry><entry><title type="html">Dominic Baril</title><link href="https://norlab.ulaval.ca/people/d_baril_fr/" rel="alternate" type="text/html" title="Dominic Baril" /><published>2019-05-05T00:00:00-04:00</published><updated>2019-05-05T00:00:00-04:00</updated><id>https://norlab.ulaval.ca/people/d_baril_fr</id><content type="html" xml:base="https://norlab.ulaval.ca/people/d_baril_fr/">&lt;p&gt;Dominic Baril est un étudiant à la maîtrise au sein du Norlab.
Il a gardué du baccalauréat en génie mécanique à l’Université de Sherbrooke en 2018. Afin de compléter son baccalauréat, il a participé à un projet majeur en robotique en collaboration avec &lt;a href=&quot;https://www.exonetik.com/&quot; target=&quot;_blank&quot;&gt;Exonetik&lt;/a&gt;, une entreprise qui produit des actionneurs à fluides magnétorhéologiques. Le projet (dénommé &lt;a href=&quot;https://www.facebook.com/expomegageniale/videos/2039870909411790/&quot; target=&quot;_blank&quot;&gt;ASIMOV&lt;/a&gt;) consistait en la conception, la fabrication et le contrôle de 2 bras robotiques identiques et mécaniquement indépendants. Les bras sont capables de simultanément reproduire un sentiment haptique dans l’autre lorsqu’ils touchent à des objets. Il a également réalisé un stage au sein du laboratoire &lt;a href=&quot;https://https://www.createk.co/&quot; target=&quot;_blank&quot;&gt;Createk&lt;/a&gt; sous la supervision du Pr. &lt;a href=&quot;https://alexandregirard.ca/&quot; target=&quot;_blank&quot;&gt;Alexandre Girard&lt;/a&gt;. Il avait la tâche d’intégrer une voiture robotique à échelle 1:10 (basée sur le &lt;a href=&quot;http://fast.scripts.mit.edu/racecar/&quot; target=&quot;_blank&quot;&gt;RACECAR&lt;/a&gt; du MIT). La voiture a été équipée avec les capteurs standards des voitures autonomes et Dominic a réalisé un architecture de contrôle (fonctionnant avec ROS) pour le robot.
Son travail au Norlab sera en lien avec le contrôle d’un robot autonome en environnement hivernal difficile.&lt;/p&gt;

&lt;h1 id=&quot;education&quot;&gt;Education&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Baccalauréat en génie mécanique à l’Université de Sherbrooke, 2018&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Dominic Baril</name><email>dominic.baril.1@ulaval.ca</email></author><summary type="html">Dominic Baril est un étudiant à la maîtrise au sein du Norlab. Il a gardué du baccalauréat en génie mécanique à l’Université de Sherbrooke en 2018. Afin de compléter son baccalauréat, il a participé à un projet majeur en robotique en collaboration avec Exonetik, une entreprise qui produit des actionneurs à fluides magnétorhéologiques. Le projet (dénommé ASIMOV) consistait en la conception, la fabrication et le contrôle de 2 bras robotiques identiques et mécaniquement indépendants. Les bras sont capables de simultanément reproduire un sentiment haptique dans l’autre lorsqu’ils touchent à des objets. Il a également réalisé un stage au sein du laboratoire Createk sous la supervision du Pr. Alexandre Girard. Il avait la tâche d’intégrer une voiture robotique à échelle 1:10 (basée sur le RACECAR du MIT). La voiture a été équipée avec les capteurs standards des voitures autonomes et Dominic a réalisé un architecture de contrôle (fonctionnant avec ROS) pour le robot. Son travail au Norlab sera en lien avec le contrôle d’un robot autonome en environnement hivernal difficile.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22feature%22=%3E%22/people/d_baril.jpg%22,%20%22teaser%22=%3E%22/people/d_baril_avatar.jpg%22%7D" /></entry><entry><title type="html">Dominic Baril</title><link href="https://norlab.ulaval.ca/people/d_baril/" rel="alternate" type="text/html" title="Dominic Baril" /><published>2019-05-05T00:00:00-04:00</published><updated>2019-05-05T00:00:00-04:00</updated><id>https://norlab.ulaval.ca/people/d_baril</id><content type="html" xml:base="https://norlab.ulaval.ca/people/d_baril/">&lt;p&gt;Dominic Baril is a master’s student at Norlab.
He graduated in mechanical engineering at Sherbrooke University in 2018. To complete his bachelor’s, he worked on a major robotic project in collaboration with &lt;a href=&quot;https://www.exonetik.com/&quot; target=&quot;_blank&quot;&gt;Exonetik&lt;/a&gt;, a company that produces magnetorheological actuators. The project (named &lt;a href=&quot;https://www.facebook.com/expomegageniale/videos/2039870909411790/&quot; target=&quot;_blank&quot;&gt;ASIMOV&lt;/a&gt;) consisted in designing, manufacturing and controlling 2 identic and mechanically independent robotic arms. The arms are able to simultaneously reprodouce a haptic feeling in the other when touching objects. He also realised an internship in the &lt;a href=&quot;https://https://www.createk.co/&quot; target=&quot;_blank&quot;&gt;Createk&lt;/a&gt; laboratory under the supervision of Pr. &lt;a href=&quot;https://alexandregirard.ca/&quot; target=&quot;_blank&quot;&gt;Alexandre Girard&lt;/a&gt;. He was tasked with integrating a 1:10 scaled robotic car (based on the MIT’s &lt;a href=&quot;http://fast.scripts.mit.edu/racecar/&quot; target=&quot;_blank&quot;&gt;RACECAR&lt;/a&gt;). The car was outfitted with the standard autonomous car sensors and Dominic created a control architecture (using ROS) for the robot. 
His work in Norlab will be related to the control of an autonomous robot in a harsh winter environment.&lt;/p&gt;

&lt;h1 id=&quot;education&quot;&gt;Education&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Bachelor’s degree in mechanical Engineering at Sherbrooke University, 2018&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Dominic Baril</name><email>dominic.baril.1@ulaval.ca</email></author><summary type="html">Dominic Baril is a master’s student at Norlab. He graduated in mechanical engineering at Sherbrooke University in 2018. To complete his bachelor’s, he worked on a major robotic project in collaboration with Exonetik, a company that produces magnetorheological actuators. The project (named ASIMOV) consisted in designing, manufacturing and controlling 2 identic and mechanically independent robotic arms. The arms are able to simultaneously reprodouce a haptic feeling in the other when touching objects. He also realised an internship in the Createk laboratory under the supervision of Pr. Alexandre Girard. He was tasked with integrating a 1:10 scaled robotic car (based on the MIT’s RACECAR). The car was outfitted with the standard autonomous car sensors and Dominic created a control architecture (using ROS) for the robot. His work in Norlab will be related to the control of an autonomous robot in a harsh winter environment.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22feature%22=%3E%22/people/d_baril.jpg%22,%20%22teaser%22=%3E%22/people/d_baril_avatar.jpg%22%7D" /></entry><entry><title type="html">How to set a static transform with our interactive RVIZ tool</title><link href="https://norlab.ulaval.ca/research/how-to-visually-setup-static-tf/" rel="alternate" type="text/html" title="How to set a static transform with our interactive RVIZ tool" /><published>2019-04-30T00:00:00-04:00</published><updated>2019-04-30T00:00:00-04:00</updated><id>https://norlab.ulaval.ca/research/how-to-visually-setup-static-tf</id><content type="html" xml:base="https://norlab.ulaval.ca/research/how-to-visually-setup-static-tf/">&lt;p&gt;Sometimes, there is a need to quickly set up a static transform in ROS.
To avoid manually searching for translation and rotation parameters, we have written this minimalistic tool that allows us to do it a more convenient, click-and-drag way.&lt;/p&gt;

&lt;h1 id=&quot;the-problem&quot;&gt;The Problem&lt;/h1&gt;

&lt;p&gt;The physical configuration of a robot can change or a new sensor is added.
Since this configuration is reflected by the ROS TF system, the corresponding static transform needs to be updated.
The classical approach would be trial-and-error with the comand line &lt;code class=&quot;highlighter-rouge&quot;&gt;rosrun tf static_transform_publisher [params]&lt;/code&gt;, looking for the right numbers.
It is tedious, unnecessary and there is a better way to do this.&lt;/p&gt;

&lt;h1 id=&quot;solution---interactive-static-transform-tool&quot;&gt;Solution - Interactive Static Transform tool&lt;/h1&gt;

&lt;p&gt;Following the &lt;a href=&quot;http://wiki.ros.org/rviz/Tutorials/Interactive%20Markers%3A%20Getting%20Started&quot;&gt;RVIZ Interactive Marker Tutorial&lt;/a&gt;, a simple tool was written.
It helps in finding a transformation between two TF frames by providing
an RVIZ interactive marker. 
Its usage is straightforward:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Set your parent and child frame names in the &lt;a href=&quot;https://github.com/norlab-ulaval/norlab_imu_tools/blob/master/launch/interactive_static_transform_publisher.launch&quot;&gt;launch file&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Run the launch file and also open the RVIZ.&lt;/li&gt;
  &lt;li&gt;The initial transformation is an identity, but you can modify it by displaying
the interactive marker and moving it around.&lt;/li&gt;
  &lt;li&gt;The marker also offers a context menu, the only command which is there
prints the current transform value in the form of the &lt;code class=&quot;highlighter-rouge&quot;&gt;static_transform_publisher&lt;/code&gt;
rosrun command.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The launch file and the code is placed within the &lt;a href=&quot;https://github.com/norlab-ulaval/norlab_imu_tools&quot;&gt;norlab_imu_tools&lt;/a&gt; package:&lt;/p&gt;

&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;launch&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;node&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;pkg=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;norlab_imu_tools&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;interactive_static_transform_publisher.py&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;interactive_static_transform_publisher_node&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;output=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;screen&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;param&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;child_frame&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;value=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;moving_frame&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;param&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;parent_frame&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;value=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;base_link&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;param&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tf_publish_period_in_sec&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;value=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;0.1&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;/node&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/launch&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The final transform output can be copied and pasted into a terminal or just used to set a new launch file:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[INFO] [1556661102.899894]: The equivalent static transform command:
[INFO] [1556661102.902316]: rosrun tf static_transform_publisher 1.92549085617 0.0 1.12673556805 0.327530413866 -0.155546739697 -0.399795621634 0.841839015484 base_link moving_frame 100
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Vladimír Kubelka</name><email>vladimir.kubelka.1@ulaval.ca</email></author><category term="howto" /><category term="ros" /><category term="transform" /><category term="rviz" /><summary type="html">Sometimes, there is a need to quickly set up a static transform in ROS. To avoid manually searching for translation and rotation parameters, we have written this minimalistic tool that allows us to do it a more convenient, click-and-drag way.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22teaser%22=%3E%22/rviz_interactive_tf_tool_teaser.jpg%22,%20%22feature%22=%3E%22/rviz_interactive_tf_tool.jpg%22%7D" /></entry><entry><title type="html">Forest Mapping</title><link href="https://norlab.ulaval.ca/research/forest-mapping/" rel="alternate" type="text/html" title="Forest Mapping" /><published>2019-04-17T00:00:00-04:00</published><updated>2019-04-17T00:00:00-04:00</updated><id>https://norlab.ulaval.ca/research/forest-mapping</id><content type="html" xml:base="https://norlab.ulaval.ca/research/forest-mapping/">&lt;h1 id=&quot;quick-facts&quot;&gt;Quick facts:&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;1.4 hectares of forest mapped with a &lt;a href=&quot;https://www.clearpathrobotics.com/husky-unmanned-ground-vehicle-robot/&quot;&gt;Clearpath Husky&lt;/a&gt; and a &lt;a href=&quot;https://velodynelidar.com/hdl-32e.html&quot;&gt;Velodyne HDL-32E&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;943 trees were manually segmented in point clouds, and their diameter and species was identified on the field.&lt;/li&gt;
  &lt;li&gt;Biggest dataset for 3D forest mapping and tree diameter measurements&lt;/li&gt;
  &lt;li&gt;We tested various diameter estimation methods&lt;/li&gt;
  &lt;li&gt;Part of the &lt;a href=&quot;https://www.researchgate.net/project/Automated-forestry-and-logging-operations&quot;&gt;Automated Forestry and Logging Operations project&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Our preprint is available &lt;a href=&quot;https://arxiv.org/abs/1904.05281&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;video&quot;&gt;Video&lt;/h1&gt;
&lt;!-- blank line --&gt;
&lt;figure class=&quot;video_container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/dJ8eIOvcGPw&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/figure&gt;
&lt;!-- blank line --&gt;

&lt;h1 id=&quot;publications&quot;&gt;Publications&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot; reversed=&quot;reversed&quot;&gt;&lt;li&gt;&lt;span id=&quot;Tremblay2019&quot;&gt;Tremblay, J.-F., Béland, M., Pomerleau, F., Gagnon, R., &amp;amp; Giguère, P. (2019). Automatic 3D Mapping for Tree Diameter Measurements in Inventory Operations. In &lt;i&gt;preprint, ArXiv. Submitted to Field and Service Robotics (FSR)&lt;/i&gt;.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Tremblay2019/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Jean-François Tremblay</name></author><category term="project" /><category term="mapping" /><category term="forestry" /><category term="lidar" /><summary type="html">Quick facts: 1.4 hectares of forest mapped with a Clearpath Husky and a Velodyne HDL-32E 943 trees were manually segmented in point clouds, and their diameter and species was identified on the field. Biggest dataset for 3D forest mapping and tree diameter measurements We tested various diameter estimation methods Part of the Automated Forestry and Logging Operations project Our preprint is available here</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22feature%22=%3E%22projects/forest_mapping_feature.jpg%22,%20%22teaser%22=%3E%22projects/forest_mapping_teaser.jpg%22,%20%22thumb%22=%3Enil%7D" /></entry><entry><title type="html">Three articles accepted at ICRA 2019!</title><link href="https://norlab.ulaval.ca/news/icra19/" rel="alternate" type="text/html" title="Three articles accepted at ICRA 2019!" /><published>2019-03-04T00:00:00-05:00</published><updated>2019-03-04T00:00:00-05:00</updated><id>https://norlab.ulaval.ca/news/icra19</id><content type="html" xml:base="https://norlab.ulaval.ca/news/icra19/">&lt;p&gt;We are happy to announce that we have three accepted publications at the &lt;a href=&quot;https://www.icra2019.org/&quot;&gt;2019 International Conference on Robotics and Automation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We will present our latest results on lidar modeling &lt;a href=&quot;#Laconte2019&quot;&gt;(Laconte, Deschênes, Labussière, &amp;amp; Pomerleau, 2019)&lt;/a&gt;, learning algorithm for covariance estimation &lt;a href=&quot;#Landry2019&quot;&gt;(Landry, Pomerleau, &amp;amp; Giguère, 2019)&lt;/a&gt;, and a survey on different robust cost functions applied to ICP (Iterative Closest Point) &lt;a href=&quot;#Babin2019&quot;&gt;(Babin, Giguère, &amp;amp; Pomerleau, 2019)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;More details, and hopefully videos to come!&lt;/p&gt;

&lt;h1 id=&quot;our-articles&quot;&gt;Our articles&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot; reversed=&quot;reversed&quot;&gt;&lt;li&gt;&lt;span id=&quot;Landry2019&quot;&gt;Landry, D., Pomerleau, F., &amp;amp; Giguère, P. (2019). CELLO-3D: Estimating the Covariance of ICP in the Real World. In &lt;i&gt;Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)&lt;/i&gt;.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Landry2019/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Babin2019&quot;&gt;Babin, P., Giguère, P., &amp;amp; Pomerleau, F. (2019). Analysis of Robust Functions for Registration Algorithms. In &lt;i&gt;Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)&lt;/i&gt;.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Babin2019/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Laconte2019&quot;&gt;Laconte, J., Deschênes, S.-P., Labussière, M., &amp;amp; Pomerleau, F. (2019). Lidar Measurement Bias Estimation via Return Waveform Modelling in a Context of 3D Mapping. In &lt;i&gt;Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)&lt;/i&gt;.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Laconte2019/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>François Pomerleau</name><email>francois.pomerleau@ift.ulaval.ca</email></author><category term="conference" /><summary type="html">We are happy to announce that we have three accepted publications at the 2019 International Conference on Robotics and Automation.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22teaser%22=%3E%22/news/icra19/icra19_teaser.jpg%22%7D" /></entry><entry><title type="html">Vladimír Kubelka</title><link href="https://norlab.ulaval.ca/people/v_kubelka/" rel="alternate" type="text/html" title="Vladimír Kubelka" /><published>2019-02-21T00:00:00-05:00</published><updated>2019-02-21T00:00:00-05:00</updated><id>https://norlab.ulaval.ca/people/v_kubelka</id><content type="html" xml:base="https://norlab.ulaval.ca/people/v_kubelka/">&lt;p&gt;Vladimír has received his master’s degree in Cybernetics and Robotics from the Czech Technical University in Prague (2013). The experiments for the master’s thesis were performed at the ASL lab (ETH Zurich) during his visiting student internship. After that, he continued as a Ph.D. student at CTU and focused on the problem of data fusion and state estimation for ground robots in harsh conditions. He had the opportunity to participate in two EU-funded  search and rescue projects &lt;a href=&quot;http://www.nifti.eu/&quot; target=&quot;_blank&quot;&gt;NIFTi&lt;/a&gt; and &lt;a href=&quot;http://www.tradr-project.eu/&quot; target=&quot;_blank&quot;&gt;TRADR&lt;/a&gt;. These projects offered real-world scenarios to test the localization algorithms. The main challenge were sensor outages (because of dark areas, smoke), unstable terrain and semi-structured environments (e.g., earthquake aftermath). He defended his Ph.D. thesis in 2018 (supervised by &lt;a href=&quot;https://sites.google.com/site/reinsmic&quot; target=&quot;_blank&quot;&gt;Michal Reinstein&lt;/a&gt; and &lt;a href=&quot;http://cmp.felk.cvut.cz/~svoboda/&quot; target=&quot;_blank&quot;&gt;Tomáš Svoboda&lt;/a&gt;) and enrolled as a postdoc fellow with the NORLAB. The Canadian winter brings new challenges for the ground mobile robots: deep snow, adversary conditions for optical sensors and changing terrain caused by wind and blizzards.&lt;/p&gt;

&lt;p&gt;His research topics are sensor fusion and state estimation for mobile ground robots. He is interested in the problems related to deployment of robots in harsh environments.&lt;/p&gt;

&lt;h1 id=&quot;education&quot;&gt;Education&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://drive.google.com/file/d/12CeiSDOmbEq74qiiiJG8GhfCNXyB24Cp/view?usp=sharing&quot; target=&quot;_blank&quot;&gt;Ph.D.&lt;/a&gt; in Artificial Intelligence and Biocybernetics - &lt;a href=&quot;https://www.cvut.cz/en&quot; target=&quot;_blank&quot;&gt;CTU in Prague&lt;/a&gt;, 2018&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://drive.google.com/file/d/1kA04kmKFjoW7k5Jas_8xn0ivKFFsQRWb/view?usp=sharing&quot; target=&quot;_blank&quot;&gt;Ing.&lt;/a&gt; in Cybernetics and Robotics (Air and Space Systems) - &lt;a href=&quot;https://www.cvut.cz/en&quot; target=&quot;_blank&quot;&gt;CTU in Prague&lt;/a&gt;, 2013&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://drive.google.com/file/d/18vtdAQDF4s1qFMwPujufjYuyXQ_VlfzI/view?usp=sharing&quot; target=&quot;_blank&quot;&gt;Bc.&lt;/a&gt; in Electrical Engineering (Cybernetics and Measurement) - &lt;a href=&quot;https://www.cvut.cz/en&quot; target=&quot;_blank&quot;&gt;CTU in Prague&lt;/a&gt;, 2011&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;publications&quot;&gt;Publications&lt;/h1&gt;

&lt;h2 class=&quot;bibliography&quot;&gt;Journal Articles&lt;/h2&gt;
&lt;ol class=&quot;bibliography&quot; reversed=&quot;reversed&quot;&gt;&lt;li&gt;&lt;span id=&quot;kubelka2016improving&quot;&gt;Kubelka, V., Reinstein, M., &amp;amp; Svoboda, T. (2016). Improving multimodal data fusion for mobile robots by trajectory smoothing. &lt;i&gt;Robotics and Autonomous Systems&lt;/i&gt;, &lt;i&gt;84&lt;/i&gt;, 88–96.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/kubelka2016improving/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;simanek2015evaluation&quot;&gt;Simanek, J., Reinstein, M., &amp;amp; Kubelka, V. (2015). Evaluation of the EKF-based estimation architectures for data fusion in mobile robots. &lt;i&gt;IEEE/ASME Transactions on Mechatronics&lt;/i&gt;, &lt;i&gt;20&lt;/i&gt;(2), 985–990.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/simanek2015evaluation/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;simanek2015improving&quot;&gt;Simanek, J., Kubelka, V., &amp;amp; Reinstein, M. (2015). Improving multi-modal data fusion by anomaly detection. &lt;i&gt;Autonomous Robots&lt;/i&gt;, &lt;i&gt;39&lt;/i&gt;(2), 139–154.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/simanek2015improving/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Pomerleau2015e&quot;&gt;Kubelka, V., Oswald, L., Pomerleau, F., Colas, F., Svoboda, T., &amp;amp; Reinstein, M. (2015). Robust data fusion of multimodal sensory information for mobile robots. &lt;i&gt;Journal of Field Robotics&lt;/i&gt;, &lt;i&gt;32&lt;/i&gt;(4), 447–473.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Pomerleau2015e/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
&lt;h2 class=&quot;bibliography&quot;&gt;Conference Articles&lt;/h2&gt;
&lt;ol class=&quot;bibliography&quot; reversed=&quot;reversed&quot;&gt;&lt;li&gt;&lt;span id=&quot;Dandurand2019&quot;&gt;Dandurand, P., Babin, P., Kubelka, V., Giguère, P., &amp;amp; Pomerleau, F. (2019). Predicting GNSS satellite visibility from dense point clouds. In &lt;i&gt;preprint, ArXiv. Submitted to Field and Service Robotics (FSR)&lt;/i&gt;.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Dandurand2019/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Babin2019a&quot;&gt;Babin, P., Dandurand, P., Kubelka, V., Giguère, P., &amp;amp; Pomerleau, F. (2019). Large-scale 3D Mapping of Sub-arctic Forests. In &lt;i&gt;preprint, ArXiv. Submitted to Field and Service Robotics (FSR)&lt;/i&gt;.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Babin2019a/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kruijff2016deployment&quot;&gt;Kruijff-Korbayová, I., Freda, L., Gianni, M., Ntouskos, V., Hlaváč, V., Kubelka, V., … others. (2016). Deployment of ground and aerial robots in earthquake-struck amatrice in italy (brief report). In &lt;i&gt;2016 IEEE international symposium on safety, security, and rescue robotics (SSRR)&lt;/i&gt; (pp. 278–279). IEEE.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/kruijff2016deployment/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;jirkuu2016wifi&quot;&gt;Jirku, M., Kubelka, V., &amp;amp; Reinstein, M. (2016). WiFi localization in 3D. In &lt;i&gt;2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)&lt;/i&gt; (pp. 4551–4557). IEEE.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/jirkuu2016wifi/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kubelka2014combining&quot;&gt;Kubelka, V., &amp;amp; Reinstein, M. (2014). Combining Complementary Motion Estimation Approaches to Increase Reliability in Urban Search &amp;amp; Rescue Missions. In &lt;i&gt;International Workshop on Modelling and Simulation for Autonomous Systems&lt;/i&gt; (pp. 347–356). Springer, Cham.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/kubelka2014combining/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;reinstein2013terrain&quot;&gt;Reinstein, M., Kubelka, V., &amp;amp; Zimmermann, K. (2013). Terrain adaptive odometry for mobile skid-steer robots. In &lt;i&gt;2013 IEEE International Conference on Robotics and Automation&lt;/i&gt; (pp. 4706–4711). IEEE.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/reinstein2013terrain/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kubelka2012complementary&quot;&gt;Kubelka, V., &amp;amp; Reinstein, M. (2012). Complementary filtering approach to orientation estimation using inertial sensors only. In &lt;i&gt;2012 IEEE international conference on robotics and automation&lt;/i&gt; (pp. 599–605). IEEE.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/kubelka2012complementary/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Vladimír Kubelka</name><email>vladimir.kubelka.1@ulaval.ca</email></author><summary type="html">Vladimír has received his master’s degree in Cybernetics and Robotics from the Czech Technical University in Prague (2013). The experiments for the master’s thesis were performed at the ASL lab (ETH Zurich) during his visiting student internship. After that, he continued as a Ph.D. student at CTU and focused on the problem of data fusion and state estimation for ground robots in harsh conditions. He had the opportunity to participate in two EU-funded search and rescue projects NIFTi and TRADR. These projects offered real-world scenarios to test the localization algorithms. The main challenge were sensor outages (because of dark areas, smoke), unstable terrain and semi-structured environments (e.g., earthquake aftermath). He defended his Ph.D. thesis in 2018 (supervised by Michal Reinstein and Tomáš Svoboda) and enrolled as a postdoc fellow with the NORLAB. The Canadian winter brings new challenges for the ground mobile robots: deep snow, adversary conditions for optical sensors and changing terrain caused by wind and blizzards.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22feature%22=%3E%22/people/v_kubelka.jpg%22,%20%22teaser%22=%3E%22/people/v_kubelka_avatar.jpg%22%7D" /></entry><entry><title type="html">Enquête sur l’utilisation des lidars avec les UAV et la cartographie souterraine</title><link href="https://norlab.ulaval.ca/publications/survey-lidar-UAV_fr/" rel="alternate" type="text/html" title="Enquête sur l'utilisation des lidars avec les UAV et la cartographie souterraine" /><published>2019-02-14T00:00:00-05:00</published><updated>2019-02-14T00:00:00-05:00</updated><id>https://norlab.ulaval.ca/publications/survey-lidar-UAV_fr</id><content type="html" xml:base="https://norlab.ulaval.ca/publications/survey-lidar-UAV_fr/">&lt;p&gt;Cette enquête présente une vue générale des projets utilisant des lidars (Technologie de mesure de distance et détection par laser) sur des UAV (Véhicule Aérien Autonome).&lt;/p&gt;

&lt;p&gt;De nos jours le nombre de projets utilisant des UAV avec des lidars est en forte augmentation. 
Du fait de leurs capacités à accéder à des endroits inaccessibles pour l’homme et de leur facilité de déploiement et d’utilisation, les UAV sont plus à même de donner des points de vue différents intéressant que les UGV (Véhicule Terrestre Autonome). 
Cependant, l’utilisation de lidar sur les UAV reste encore un défi en soi et beaucoup de problèmes restent encore à surmonter. 
L’estimation de la position de l’UAV, sa vitesse et son altitude, les caractéristiques de ses équipements et les algorithmes utilisés influencent significativement la précision des mesures de cartographie. 
Dans cette enquête, nous présentons une liste des articles utilisant les lidars sur les UAV et donnons quelques détails à propos des équipements et algorithmes utilisés pour leur projet. 
L’objectif est de fournir des éléments de comparaison sur les équipements utilisés dans les différents projets afin de faciliter le choix d’un lidar pour la cartographie avec des UAV. 
Étant donné le nombre important d’articles sur le sujet, nous divisons cette enquête en sept catégories:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#cartographie-souterraine&quot;&gt;cartographie souterraine&lt;/a&gt;,&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#mesure-avec-dirigeable&quot;&gt;mesure avec dirigeable&lt;/a&gt;,&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#estimation-de-la-pose-et-trajectoire&quot;&gt;estimation de la position et trajectoire avec lidar&lt;/a&gt;,&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#mesure-quelques-caractéristiques-du-paysage&quot;&gt;mesure de quelques caractéristiques du paysage&lt;/a&gt;,&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#mesure-objets&quot;&gt;mesure d’objets&lt;/a&gt;,&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lidar-sur-UAV&quot;&gt;test de lidar sur UAV&lt;/a&gt;, et finallement&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#algorithmes-pour-odometry-et-comparaison-entre-lidar-sur-uav-et-ugv&quot;&gt;algorithmes pour l’odométrie et comparaison entre lidar sur UAV et lidar sur robot terrestre&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Cette enquête est en cours de rédaction. Elle sera régulièrement mise à jour.
Nous espérons que ces informations serons utiles à la communauté scientifique.
Si vous remarquer l’absence de l’un de vos articles ou une erreur, n’hésitez pas à nous contacter.&lt;/p&gt;

&lt;p&gt;Légende: “O” signifie présent et “-“ signifie absent de l’article.&lt;/p&gt;

&lt;h3 id=&quot;cartographie-souterraine&quot;&gt;Cartographie souterraine&lt;/h3&gt;
&lt;div style=&quot;overflow-x: visible;&quot;&gt;
&lt;table class=&quot;large&quot;&gt;
	&lt;thead&gt;
		&lt;tr&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Article&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Sujet&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;UAV&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;lidar&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Caméra&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;GPS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;IMU&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;GNSS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;INS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Test&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Notes&lt;/th&gt;
		&lt;/tr&gt;
	&lt;/thead&gt;
	&lt;tbody&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Michael2012&quot;&gt;(Michael et al., 2012)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Cartographie 3D d'un bâtiments endommagé par un tremblement de terre en utilisant une collaboration entre un robot terrestre et aérien&lt;/td&gt;
			&lt;td&gt;Pelican quadrotor&lt;/td&gt;
			&lt;td&gt;Hokuyo TM-30LX&lt;/td&gt;
			&lt;td&gt;Microsoft Kinect&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Intérieur&lt;/td&gt;
			&lt;td&gt;UAV peut se déployer à partir de l'UGV, ICP pour cartographie, SLAM pour navigation&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Liu2015&quot;&gt;(Liu, Mohta, Shen, &amp;amp; Kumar, 2015)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Multiple UAV pour cartographie 3D&lt;/td&gt;
			&lt;td&gt;Pelican quadrotor&lt;/td&gt;
			&lt;td&gt;Hokuyo UTM-30LX&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Simulation et test intérieur&lt;/td&gt;
			&lt;td&gt;SLAM pour navigation et plannification de trajectoire&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#AjayKumar2017&quot;&gt;(Ajay Kumar, Patil, Patil, Park, &amp;amp; Chai, 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Reconstruction d'intérieur de bâtiment et classification de tuyaux&lt;/td&gt;
			&lt;td&gt;Phantom 3 quadcopter avancé&lt;/td&gt;
			&lt;td&gt;Hokuyo UTM-30LX, Hokuyo URG-04LX-UG01&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Intérieur&lt;/td&gt;
			&lt;td&gt;ICP pour fusionner les données, utilisation d'un filtre de Kalman pour avoir la position, classification des tuyaux en calculant leur courbature&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Edwards2017&quot;&gt;(Edwards, 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Scan d'un tunnel de mine&lt;/td&gt;
			&lt;td&gt;Asctec Hummingbird&lt;/td&gt;
			&lt;td&gt;Z+F Imager S010 3D (terrestrial)&lt;/td&gt;
			&lt;td&gt;Xtion Pro live &lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Intérieur&lt;/td&gt;
			&lt;td&gt;Comparaison entre l'algorithme Microsoft Kinect Fusion et un lidar terrestre&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Vempati2017&quot;&gt;(Vempati, Gilitschenski, Nieto, Beardsley, &amp;amp; Siegwart, 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Cartographie 3D intérieur en temps-réel avec UAV&lt;/td&gt;
			&lt;td&gt;DJI M100&lt;/td&gt;
			&lt;td&gt;Leica Multistation MS50 (sert de référence)&lt;/td&gt;
			&lt;td&gt;Intel RealSense R200&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;capteur DUO-MLX visual-inertial&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Simulation and test intérieur&lt;/td&gt;
			&lt;td&gt;SLAM avec GPU parallélisé pour gérer ses ressources mémoire, 5mm de résolution à 60Hz, filtre de Kalman étendu, ICP pour cartographie&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Ozaslan2017&quot;&gt;(Ozaslan et al., 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Inspection de tunnel et pipeline avec UAV&lt;/td&gt;
			&lt;td&gt;DJI F550&lt;/td&gt;
			&lt;td&gt;Velodyne Puck Lite&lt;/td&gt;
			&lt;td&gt;MChameleon 3 caméra&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;PickHawk&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Intérieur&lt;/td&gt;
			&lt;td&gt;Cartographie local pour l'estimation de la pose&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Mascarich2018&quot;&gt;(Mascarich et al., 2018)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Inspection de site nucléaire et tunnel avec UAV&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;Capteur de profondeur de type Time of flight&lt;/td&gt;
			&lt;td&gt;Chameleon 3 caméra&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Intérieur&lt;/td&gt;
			&lt;td&gt;Détection des radiations et de leur localisation&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Papachristos2019&quot;&gt;(Papachristos, Khattak, Mascarich, &amp;amp; Alexis, 2019)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Exploration autonome et cartographie de mines souterraines&lt;/td&gt;
			&lt;td&gt;Quadrotor&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Intérieur&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
		&lt;/tr&gt;
	&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h3 id=&quot;mesure-avec-dirigeable&quot;&gt;Mesure avec dirigeable&lt;/h3&gt;
&lt;div style=&quot;overflow-x: visible;&quot;&gt;
&lt;table class=&quot;large&quot;&gt;
	&lt;thead&gt;
		&lt;tr&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Article&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Subject&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;UAV&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;lidar&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Camera&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;GPS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;IMU&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;GNSS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;INS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Test&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Note&lt;/th&gt;
		&lt;/tr&gt;
	&lt;/thead&gt;
	&lt;tbody&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Pan2015&quot;&gt;(Pan et al., 2015)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Système pour suivre les lignes éléctriques afin de détecter des problèmes&lt;/td&gt;
			&lt;td&gt;Petit dirigeable à l'hélium&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Koska2017&quot;&gt;(Koska et al., 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Comparaison entre Lidar sur dirigeable et caméra sur UAV&lt;/td&gt;
			&lt;td&gt;Dirigeable ACC15X, 12m de long, 2.8m de diamètre (57m^3), 15kg de charge&lt;/td&gt;
			&lt;td&gt;Sick LD-LRS1000 (2D avec rotation)&lt;/td&gt;
			&lt;td&gt;Canon EOS 100D, FLIR SC645&lt;/td&gt;
			&lt;td&gt;IMAR iTracer-F200&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Dirigeable crée par AirshipClub.com, problème de précision, problème avec la structure mais plus précis avec le dirigeable sur de larges zones qu'avec l'UAV&lt;/td&gt;
		&lt;/tr&gt;
	&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h3 id=&quot;estimation-de-la-position-et-trajectoire-avec-lidar&quot;&gt;Estimation de la position et trajectoire avec lidar&lt;/h3&gt;
&lt;div style=&quot;overflow-x: visible;&quot;&gt;
&lt;table class=&quot;large&quot;&gt;
	&lt;thead&gt;
		&lt;tr class=&quot;header&quot;&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Article&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Sujet&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;UAV&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;lidar&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Caméra&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;GPS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;IMU&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;GNSS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;INS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Test&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Notes&lt;/th&gt;
		&lt;/tr&gt;
	&lt;/thead&gt;
	&lt;tbody&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Downs2004&quot;&gt;(Downs, Madhavan, &amp;amp; Hong, 2004)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Meilleure estimation de la position de l'UAV avec un lidar que par GPS&lt;/td&gt;
			&lt;td&gt;Flying Eye&lt;/td&gt;
			&lt;td&gt;LADAR system (UAV)/LMS-Z420i au sol&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;ICP pour traiter 2 séries de mesure, comparaison des données de UAV à celles au sol pour mesurer la précision&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Hwang2012&quot;&gt;(Hwang &amp;amp; Tahk, 2012)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Mesure de la position d'un UAV avec lidar&lt;/td&gt;
			&lt;td&gt;Simualation d'un avion&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Simulation afin de comparer 2 algorithmiques pour localisation&lt;/td&gt;
			&lt;td&gt;Le lidar mesure le relief, comparaison entre le filtre de Kalman et la Cross Correlation&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Han2012&quot;&gt;(Han et al., 2012)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Problème de la mesure de la position d'une cible par apport à un drone&lt;/td&gt;
			&lt;td&gt;Petit avion&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Images extraites d'une base de données pour tester l'algorithme de détection de contours, GPS+IMU pour estimer la position de l'UAV, ICP pour traiter les données, discussion sur le nombre de cycle d'ICP et leur influence sur les résultats&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Gee2017&quot;&gt;(Gee, James, Van Der Mark, Delmas, &amp;amp; Gimel’Farb, 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Fusion de plusieurs nuages de points issus d'un lidar en vue d'utiliser l'algorithme SLAM&lt;/td&gt;
			&lt;td&gt;DJI Phantom Quadcopter&lt;/td&gt;
			&lt;td&gt;Velodyne puck VLP-16&lt;/td&gt;
			&lt;td&gt;GoPro Hero 3+&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Reconstruction des données, lidar statique au sol (caméra sur UAV), approximation de trajectoires avec les points puis ICP et SLAM&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Shetty2017&quot;&gt;(Shetty &amp;amp; Gao, 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Fusion des données GPS et lidar pour corriger l'erreur du GPS dans un environnement urbain&lt;/td&gt;
			&lt;td&gt;Quadrotor&lt;/td&gt;
			&lt;td&gt;Velodyne VLP-16 puck lite&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;LEA-6T GPS&lt;/td&gt;
			&lt;td&gt;Xsens-Mti-30 IMU&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Modélisation de l'erreur de covariance du nuage de point, pose de l'UAV par Unscented Kalaman filter, matching des données lidar avec modèle 3D d'une ville, ICP pour faire correspondre les nuages de points&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Zhen&quot;&gt;(Zhen &amp;amp; Scherer, n.d.)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Test une méthode multi-plateforme avec lidar et l'algorithme SLAM pour UAV (2D-3D)&lt;/td&gt;
			&lt;td&gt;DJI M100 DJI M600&lt;/td&gt;
			&lt;td&gt;Rotating Hokuyo Rotating Velodyne VLP-16&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Simulation et en extérieur&lt;/td&gt;
			&lt;td&gt;Kalman filter (ESKF), Gaussian Particle Filter (GPF), vitesse faible pour éviter erreur de position&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Youn2018&quot;&gt;(Youn, Kim, Kim, Yoo, &amp;amp; Lee, 2018)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Détection de l'environnement 3D pour plannifier, se déplacer et surveiller le traffic dans le ciel&lt;/td&gt;
			&lt;td&gt;Cessma 208B&lt;/td&gt;
			&lt;td&gt;ALS50-III&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Découpage du monde en cube de certaines tailles (différents niveaux de découpage suivant 20 degré de lattitude), 20 niveaux au total&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Gomes2018&quot;&gt;(Gomes, Guerreiro, Cunha, Silvestre, &amp;amp; Oliveira, 2018)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Mesure de la déviation de trajectoire avec des données issues d'un lidar sur UAV&lt;/td&gt;
			&lt;td&gt;Mikrokopter Quadro XL&lt;/td&gt;
			&lt;td&gt;Hokuyo UTM30LX&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Microstrain IMU&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Intérieur&lt;/td&gt;
			&lt;td&gt;Méthode de detection de contours avec nuage de points, estimation de la position&lt;/td&gt;
		&lt;/tr&gt;
	&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;center&gt;&lt;h3&gt;Mesure de quelques caractéristiques du paysage&lt;/h3&gt;&lt;/center&gt;
&lt;div style=&quot;overflow-x: visible;&quot;&gt;
&lt;table class=&quot;large&quot;&gt;
	&lt;thead&gt;
		&lt;tr class=&quot;header&quot;&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Article&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Sujet&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;UAV&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;lidar&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Caméra&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;GPS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;IMU&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;GNSS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;INS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Test&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Notes&lt;/th&gt;
		&lt;/tr&gt;
	&lt;/thead&gt;
	&lt;tbody&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Liu2011&quot;&gt;(Liu, Li, Lei, Liu, &amp;amp; Wu, 2011)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Analyse du sol avec UAV et lidar pour détecter et anticiper des glissements de terrain&lt;/td&gt;
			&lt;td&gt;Helicoptère&lt;/td&gt;
			&lt;td&gt;lidar sur helicoptère&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;DGPS&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Traitement sur point cloud et images&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Durst2011&quot;&gt;(Durst, Baylot, &amp;amp; McKinley, 2011)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Utilisation d'un drone et lidar pour l'étude des propiétés des routes (type de véhicule, courbature, taille,...)&lt;/td&gt;
			&lt;td&gt;Buckeye drone&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Le lidar fait la mesure de l’élévation, le chemin est repéré par un traitement sur image&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Wallace2012&quot;&gt;(Wallace, Lucieer, Watson, &amp;amp; Turner, 2012)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Recherches pour détecter des arbres dans une forêt (taille, position, diamètre)&lt;/td&gt;
			&lt;td&gt;Oktokopter Droidworsc/Micropter AD-8 (terraLuma)&lt;/td&gt;
			&lt;td&gt;Ibeo LUX laser scanner&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Filtre de Kalman pour donner la position&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Wallace2012a&quot;&gt;(Wallace, Lucieer, &amp;amp; Watson, 2012)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Mesure des effets de l'environnement au sol et des paramètres de mesures pour la prise de données sur des arbres&lt;/td&gt;
			&lt;td&gt;Drone TerraLuma&lt;/td&gt;
			&lt;td&gt;Ibeo LUX laser scanner&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Candigliota2012&quot;&gt;(Candigliota, Immordino, Moretti, &amp;amp; Indirli, 2012)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Analyse de la structure d'une ville après un tremblement de terre pour éviter de futures dégâts&lt;/td&gt;
			&lt;td&gt;Quadrotor&lt;/td&gt;
			&lt;td&gt;Leica Geosystems HDS 3000&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;DGPS&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;ICP pour traiter les données&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Zhou2013&quot;&gt;(Zhou et al., 2013)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Utilisation de différentes altitude et vitesse pour prendre des données sur un paysage&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Wallace2013&quot;&gt;(Wallace, 2013)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Test de nouvelles caractéristiques pour l'analyse de la structure d'une forêt de manière probabilistique&lt;/td&gt;
			&lt;td&gt;Drone TerraLuma&lt;/td&gt;
			&lt;td&gt;Ibeo LUX laser scanner&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Roca2014&quot;&gt;(Roca, Armesto, Lagüela, &amp;amp; Díaz-Vilariño, 2014)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Test d'un UAV avec lidar pour la mesure du sol&lt;/td&gt;
			&lt;td&gt;Hisystems GmbH&lt;/td&gt;
			&lt;td&gt;Hokuyo UTM-30LX&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Filtre de Kalman pour la position, test effectué sur une maison&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Esposito2014&quot;&gt;(Esposito et al., 2014)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Mesure des caractéristiques des arbres (taille, forme,...) pour valider le fonctionnement d'un lidar sur un UAV&lt;/td&gt;
			&lt;td&gt;Ultralight helicoptère&lt;/td&gt;
			&lt;td&gt;YellowScan lidar&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Wallace2014&quot;&gt;(Wallace, Lucieer, &amp;amp; Watson, 2014)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Mesure l'influence des algorithmes de détection et de la densité de points sur la précision de la détection des arbres et de leurs caractéristiques&lt;/td&gt;
			&lt;td&gt;Drone TerraLuma&lt;/td&gt;
			&lt;td&gt;Ibeo LUX laser scanner&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Ni2015&quot;&gt;(Ni, Liu, Zhang, Sun, &amp;amp; Yang, 2015)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Fait l'inventaire d'une forêt avec UAV et lidar&lt;/td&gt;
			&lt;td&gt;Yun-5 aircraft&lt;/td&gt;
			&lt;td&gt;Leica ALS 60&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Mandlburger2015&quot;&gt;(Mandlburger et al., 2015)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Analyse les caractéristiques d'un cours d'eau avec UAV et lidar (dynamique, délimitation,...)&lt;/td&gt;
			&lt;td&gt;Ricopter UAV&lt;/td&gt;
			&lt;td&gt;Riegl VUX-Sys, VUX-1, VQ-880-G&lt;/td&gt;
			&lt;td&gt;Sony Alpha 6000 RGB&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Reiss2016&quot;&gt;(Reiss et al., 2016)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Cartographie et enregistrement de ruines avec lidar sur UAV&lt;/td&gt;
			&lt;td&gt;Asctec Falcon 8, Sensefly e-Bee&lt;/td&gt;
			&lt;td&gt;Faro Focus 3D S-120 (UAV), Optec 3D-HD ILRIS (terrestre)&lt;/td&gt;
			&lt;td&gt;Sony Alpha 6000 RGB&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Fusion des données terrestre et aérienne par photogramétrie&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Guo2017&quot;&gt;(Guo et al., 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Cartographie de forêt avec UAV et lidar&lt;/td&gt;
			&lt;td&gt;8 rotor UAV&lt;/td&gt;
			&lt;td&gt;Velodyne puck VLP-16&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;Novatel IMU&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Calibration des équipements, obtention des différentes caractéristiques sur la forêt&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Wei2017&quot;&gt;(Wei, Yang, Jiang, Cao, &amp;amp; Wu, 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Filtrer les effets de la végétation sur les mesures de paysage&lt;/td&gt;
			&lt;td&gt;8 rotor UAV&lt;/td&gt;
			&lt;td&gt;Riegl VUX-1&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;Trimble Applanix AP20-IMU&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Chiang2017&quot;&gt;(Chiang, Tsai, Li, &amp;amp; El-Sheimy, 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Reconstruction d'un bâtiment avec UAV et lidar&lt;/td&gt;
			&lt;td&gt;Petit hélicoptère&lt;/td&gt;
			&lt;td&gt;Velodyne lidar VLP-16&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;C-Migits III&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Nouvelle stratégie ICP afin de traiter la déformation des points, Filtre de Kalman pour données, comparaison avec lidar terrestre&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Chen2017&quot;&gt;(Chen, McDermid, Castilla, &amp;amp; Linke, 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Mesure de la végétation boréale avec UAV et lidar&lt;/td&gt;
			&lt;td&gt;Quadcoptère&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Hauteur de la végétation mesurée, RTK GNSS pour la position, utilisation de données lidar dans base de données, croisements des données pour générer une carte des mesures&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Sankey2017&quot;&gt;(Sankey, Donager, McVay, &amp;amp; Sankey, 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Cartographie d'une forêt avec lidar sur UAV&lt;/td&gt;
			&lt;td&gt;Octocoptère&lt;/td&gt;
			&lt;td&gt;Velodyne HDL-32E (UAV), Riegl VZ-1000(terrestre)&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Comparaison via le lidar terrestre, détermination des espéces d'arbres présent grâce à la réflexance du laser, limiter par la densité de peuplement des arbres&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Christiansen2017&quot;&gt;(Christiansen, Laursen, Jørgensen, Skovsen, &amp;amp; Gislum, 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Cartographie et mesure de la taille de culture avec UAV et lidar&lt;/td&gt;
			&lt;td&gt;DJI Matrice-100 quadcoptère&lt;/td&gt;
			&lt;td&gt;Velodyne VLP-16&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;VN-200 IMU&lt;/td&gt;
			&lt;td&gt;Trimble BD920 GNSS&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Traitement des données pour isoler le champ puis mesurer sa hauteur, dépend grandement des valeurs du GPS&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Gawel2017&quot;&gt;(Gawel et al., 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Fait la cartographie d'une zone sinistré avec UAV et lidar, puis envoi ensuite les données à des robots terrestres pour utiliser l'algorithme SLAM&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Filtre de étendu Kalman, ICP pour fusionner les données, robot mobile avec lidar, fusion des données UAV et UGV&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Elaksher2017&quot;&gt;(Elaksher, Bhandari, Carreon Limones, &amp;amp; Lauf, 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Cartographie de champs avec UAV et lidar&lt;/td&gt;
			&lt;td&gt;DJI S900 hexacoptère&lt;/td&gt;
			&lt;td&gt;Velodyne VLP-16&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Classification des points (sol ou végétation)&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#PututAshShidiq2017&quot;&gt;(Putut Ash Shidiq et al., 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Topographie de forêt avec UAV et lidar afin de différencier le sol des arbres&lt;/td&gt;
			&lt;td&gt;DJI Matrice-600&lt;/td&gt;
			&lt;td&gt;YellowScan lidar&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Morsdorf2017&quot;&gt;(Morsdorf et al., 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Topographie de forêt avec lidar sur UAV et comparaison avec lidar terrestre&lt;/td&gt;
			&lt;td&gt;Scout B1-100 helicoptère&lt;/td&gt;
			&lt;td&gt;Riegl VUX-1UAV (drone), Riegl VZ1000 (terrestre)&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;xTS xNAV550 GPS&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;4 m/s de vitesse, erreur de 1m sur la hauteur des arbres&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Thiel2017&quot;&gt;(Thiel &amp;amp; Schmullius, 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Comparaison entre les nuages de points obtenus par camera sur UAV et lidar sur UGV, pour trouver des caractéristiques sur les forêts&lt;/td&gt;
			&lt;td&gt;Geocopter X8000&lt;/td&gt;
			&lt;td&gt;Riegl VZ 1000 TLS (terrestre)&lt;/td&gt;
			&lt;td&gt;Sony NEX-7 RGB&lt;/td&gt;
			&lt;td&gt;DGPS&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Trame des maxima, taille canopé et niveau de détection, 8 m/s pour UAV, comparaison avec lidar terrestre, traitement des données pour avoir des caractéristiques sur les forêts&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Huang2018&quot;&gt;(Huang, Yeh, Tseng, &amp;amp; Hsu, 2018)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Mesure les propriétés des vagues de la mer, du changement du littoral et des marées avec UAV et lidar&lt;/td&gt;
			&lt;td&gt;DJI S1000&lt;/td&gt;
			&lt;td&gt;Hokuyo UTM-30LX&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Xsens-Mti-30 IMU&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;RTK GNSS, calibration et mesure des données, comparaison des données lidar avec capteur de pression Doppler Velocimetry (Son Tek ADV-Oceans)&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Cramer2018&quot;&gt;(Cramer, Haala, Laupheimer, Mandlburger, &amp;amp; Havel, 2018)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Mesure du sol et paysage avec un lidar haute précision sur UAV(3-5mm)&lt;/td&gt;
			&lt;td&gt;Ricopter multi-copter pateforme&lt;/td&gt;
			&lt;td&gt;Riegl VUX-1LR&lt;/td&gt;
			&lt;td&gt;Sony Alpha 6000 RGB&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;8 m/s de vitesse, 50m de hauteur, comparaison avec mesure au sol (tacheometrie)&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Govedarica2018&quot;&gt;(Govedarica, Jakovljevic, &amp;amp; Álvarez Taboada, 2018)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Étude du risque d'inondation avec UAV et lidar&lt;/td&gt;
			&lt;td&gt;WingtraOno drone&lt;/td&gt;
			&lt;td&gt;LMS-Q680i-Full (avion)&lt;/td&gt;
			&lt;td&gt;42 MP Sony RX1RII&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Technique DEM (digital elevation models), lidar comme référence pour caméra&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Hinterhofer2018&quot;&gt;(Hinterhofer et al., 2018)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Utilisation d'un UAV et lidar sur un site nucléaire sinistré&lt;/td&gt;
			&lt;td&gt;Ricopter-M&lt;/td&gt;
			&lt;td&gt;Riegl VUX-1UAV &lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Cartographier le terrain et mesurer le taux gamma de radiation&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Yan2018&quot;&gt;(Yan et al., 2018)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Mesure de la prodution forestière par UAV et lidar&lt;/td&gt;
			&lt;td&gt;GV 1300 multirotor &lt;/td&gt;
			&lt;td&gt;Velodyne VLP-16E&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;IMU-IGM-S1&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Segmentation des arbres par algorithme&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Liu2018&quot;&gt;(Liu, Shen, Cao, Wang, &amp;amp; Cao, 2018)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Mesure de la strutcure d'une forêt avec UAV et lidar&lt;/td&gt;
			&lt;td&gt;GV 1900 multi-rotor UAV&lt;/td&gt;
			&lt;td&gt;Velodyne puck VLP-16&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;IMU-IGM-S1&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Polewski2019&quot;&gt;(Polewski, Yao, Cao, &amp;amp; Gao, 2019)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Méthode pour obtenir la position des arbres en utilisant un nuage de points obtenu avec UAV et lidar&lt;/td&gt;
			&lt;td&gt;Quadrotor&lt;/td&gt;
			&lt;td&gt;Velodyne puck VLP-16 (UAV), velodyne puck VLP-16 (UGV)&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;GPS Novatel (UAV)&lt;/td&gt;
			&lt;td&gt;IMU Novatel SPAN-MEMS (UAV)&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;POS (UGV)&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;lidar terrestre porté par une personne sur sac à dos pendant les tests&lt;/td&gt;
		&lt;/tr&gt;
	&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;center&gt;&lt;h3&gt;Mesure d'objets&lt;/h3&gt;&lt;/center&gt;
&lt;div style=&quot;overflow-x: visible;&quot;&gt;
&lt;table class=&quot;large&quot;&gt;
	&lt;thead&gt;
		&lt;tr class=&quot;header&quot;&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Article&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Sujet&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;UAV&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;lidar&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Caméra&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;GPS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;IMU&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;GNSS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;INS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Test&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Notes&lt;/th&gt;
		&lt;/tr&gt;
	&lt;/thead&gt;
	&lt;tbody&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Lin2014&quot;&gt;(Lin &amp;amp; West, 2014)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Mesure et cartographie d'objets avec une géométrie oblique&lt;/td&gt;
			&lt;td&gt;Microdrone md4-200&lt;/td&gt;
			&lt;td&gt;Sensei MLS system (UAV), Ibeo Lux Laser (véhicule terrestre)&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Fusion des données entre UAV et UGV&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Gu2015&quot;&gt;(Gu &amp;amp; Zhang, 2015)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Localisation d'objets avec une plateforme UAV rapide&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Augmente la rapidité avec des algorithmes (K-d tree, AK-d tree), altitude trop élevée implique de moins bonne performance&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Moore2017&quot;&gt;(Moore et al., 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Inspection de lignes éléctriques avec UAV&lt;/td&gt;
			&lt;td&gt;Octorotor&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Ajout d'un lidar pour éviter une collision et plannifier un chemin, utilisation de plusieurs drones pour parcourir la ligne, transformation des données lidar en forme polygominale, évitement de collision des drones en temps réel&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Teng2017&quot;&gt;(Teng et al., 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Inspection de lignes éléctriques avec UAV&lt;/td&gt;
			&lt;td&gt;Hexarotor&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Isole les vibrations de l'UAV du lidar, détection des lignes hautes tensions affaisées&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Nikolov2017&quot;&gt;(Nikolov &amp;amp; Madsen, 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Inspection de pâles d'éoliennes avec UAV et lidar&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;RPlidar&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;BNO055g-DOF IMU&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Simplification en 2D de la forme des pâles, analyse de celles-ci&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Chen2018&quot;&gt;(Chen, Yang, Song, Peng, &amp;amp; Huang, 2018)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Mesure de la distance entre les lignes éléctriques et le sol en vue de détecter des problèmes&lt;/td&gt;
			&lt;td&gt;Drone mini helicoptère&lt;/td&gt;
			&lt;td&gt;Riegl VZ400&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;POS&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Algorithme de différentiation sol et câble, mesure courbure des câbles&lt;/td&gt;
		&lt;/tr&gt;
	&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;center&gt;&lt;h3&gt;Test de lidar sur UAV&lt;/h3&gt;&lt;/center&gt;
&lt;div style=&quot;overflow-x: visible;&quot;&gt;
&lt;table class=&quot;large&quot;&gt;
	&lt;thead&gt;
		&lt;tr class=&quot;header&quot;&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Article&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Sujet&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;UAV&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;lidar&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Caméra&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;GPS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;IMU&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;GNSS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;INS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Test&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Notes&lt;/th&gt;
		&lt;/tr&gt;
	&lt;/thead&gt;
	&lt;tbody&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Li2015&quot;&gt;(Li, Yan, Jing, &amp;amp; Zhao, 2015)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Analyse des vibrations de différents UAV pour l'utilisation d'un lidar sur ceux-ci&lt;/td&gt;
			&lt;td&gt;Gasoline helicoptère&lt;/td&gt;
			&lt;td&gt;HDL-32 lidar&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Sélection d'un rotor adapté, mesure et tentative d'atténuation des vibrations avec IMU&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Tulldahl2015&quot;&gt;(Tulldahl, Bissmarck, Larsson, Grönwall, &amp;amp; Tolt, 2015)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Évaluation de la précision d'un lidar sur UAV&lt;/td&gt;
			&lt;td&gt;Tarot-810 hexacoptère&lt;/td&gt;
			&lt;td&gt;Velodyne HDL-32E&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Traitement des données (calibration dynamique)&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Yang2015&quot;&gt;(Yang &amp;amp; Chen, 2015)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Utilisation d'un lidar sur un mini-UAV&lt;/td&gt;
			&lt;td&gt;Rotor wing mini UAV&lt;/td&gt;
			&lt;td&gt;Riegl LMS-Q160&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Traitement des données pour avoir la forme des objets repérés par le lidar (ICP), fusion de la forme, des points et images obtenu par la pose de l'UAV et lidar&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Kasturi2016&quot;&gt;(Kasturi, Milanovic, Atwood, &amp;amp; Yang, 2016)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Développement d'un mini scanner pour drone avec MEMS (composants optique et miroirs)&lt;/td&gt;
			&lt;td&gt;DJI Phantom II&lt;/td&gt;
			&lt;td&gt;Playzer&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Intérieur&lt;/td&gt;
			&lt;td&gt;Test du prototype, différents mode de scan par laser, différents mode de prise de mesure, tracking d'objets par laser, contrôle android du Playzer via Bluetooth&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Mastrangelo2018&quot;&gt;(Mastrangelo, von Niederhausern, Nelson, &amp;amp; Fuller, 2018)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Obtention et accès à des images 3D en temps réel depuis UAV&lt;/td&gt;
			&lt;td&gt;LASE drone&lt;/td&gt;
			&lt;td&gt;ASC TigerCub camera, Arete AirTrac laser, Sentech color camera, Hood Tech gimbal&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Acquisition des données avec correction de la pose&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Torresan2018&quot;&gt;(Torresan et al., 2018)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Intégration d'un lidar sur UAV pour mesurer ses performances&lt;/td&gt;
			&lt;td&gt;Hexarotor&lt;/td&gt;
			&lt;td&gt;LUX 4L&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;VN-300 GNSS&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Filtre de Kalman étendu, calibration des mesures, étude accès sur la précision et l'efficacité du GNSS&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Nasrollahi2018&quot;&gt;(Nasrollahi, Bolourian, Zhu, &amp;amp; Hammad, 2018)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Construction d'UAV équippé d'un lidar pour l'inspection de structure&lt;/td&gt;
			&lt;td&gt;DJI Matrice 100&lt;/td&gt;
			&lt;td&gt;Hokuyo UTM-30LX&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Intérieur&lt;/td&gt;
			&lt;td&gt;Mode stationnaire pour tester la cartographie 3D&lt;/td&gt;
		&lt;/tr&gt;
	&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;center&gt;&lt;h3&gt;Algorithmes pour l'odométrie et comparaison entre lidar sur UAV et lidar sur robot terrestre&lt;/h3&gt;&lt;/center&gt;
&lt;div style=&quot;overflow-x: visible;&quot;&gt;
&lt;table class=&quot;large&quot;&gt;
	&lt;thead&gt;
		&lt;tr class=&quot;header&quot;&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Article&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Sujet&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;UAV&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;lidar&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Caméra&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;GPS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;IMU&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;GNSS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;INS&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Test&lt;/th&gt;
			&lt;th style=&quot;text-align: center;&quot;&gt;Notes&lt;/th&gt;
		&lt;/tr&gt;
	&lt;/thead&gt;
	&lt;tbody&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Tulldahl2014&quot;&gt;(Tulldahl &amp;amp; Larsson, 2014)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Comparaison de données de lidar sur UAV avec lidar terrestre&lt;/td&gt;
			&lt;td&gt;Tarot-810 hexacoptère&lt;/td&gt;
			&lt;td&gt;Velodyne HDL-32E&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;MEMS-IMU&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Comparaison de données via lidar terrestre sur véhicule et statique&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Yousif2015&quot;&gt;(Yousif, Bab-Hadiashar, &amp;amp; Hoseinnezhad, 2015)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Description des algorithmes et stratégies utilisés pour l'odométrie&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Enquête&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Lawson2015&quot;&gt;(Lawson, 2015)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Fusion de nuages de points obtenus par de multiple UAV&lt;/td&gt;
			&lt;td&gt;Quadrocoptère construit&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Kinect RGB&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Intérieur avec 2 drones&lt;/td&gt;
			&lt;td&gt;Génération d'un nuage de points donné à un algorithme de fusion de données utilisant ICP, besoin de plusieurs superpositions pour avoir de meilleurs résultats&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Petras2016&quot;&gt;(Petras, Petrasova, Jeziorska, &amp;amp; Mitasova, 2016)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Utilisation de données obtenues par UAV et lidar via un logiciel appelé Grass GIS&lt;/td&gt;
			&lt;td&gt;Avion&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;Microsoft Kinect&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Plus accès sur le traitement de données, densité des points testés pour réduire les points redondants&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Gasparovic2017&quot;&gt;(Gašparović, Seletković, Berta, &amp;amp; Balenović, 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Évaluation de données prises par UAV et lidar durant des conditions non-optimales (vent, nuages, densité de points, ...)&lt;/td&gt;
			&lt;td&gt;DJI Phantom 4 Pro, avion Pilatus P6&lt;/td&gt;
			&lt;td&gt;Optech ALTM Gemini 167 (airplane)&lt;/td&gt;
			&lt;td&gt;FC6310 caméra (drone)&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Corrélation photo et lidar, comparaison données lidar avec nuages de points issus de photo&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Persad2017&quot;&gt;(Persad &amp;amp; Armenakis, 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Comparaison d'approche de fusion de données collectées par différents systèmes (lidar sur UAV et lidar terrestre)&lt;/td&gt;
			&lt;td&gt;Geo-X8000&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Les résultats dépendent beaucoup de la configuration et des paramètres des nuages de points&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Kwon2017&quot;&gt;(Kwon, Park, Moon, Jung, &amp;amp; Park, 2017)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Fusion rapide de données 3D atypiques dans différentes situations sur des chantiers de constructions (lidar sur UAV and lidar terrestre)&lt;/td&gt;
			&lt;td&gt;DJI Phantom 3 avanced&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;RTK pour estimation de position&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Fuad2018&quot;&gt;(Fuad et al., 2018)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Influence de l'altitude sur la précision des mesures&lt;/td&gt;
			&lt;td&gt;AL3 S1000&lt;/td&gt;
			&lt;td&gt;AL3-32&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;Plus l'altitude augmente (20m, 40m, 60m), plus la précision diminue (RMS 0.323m, 0.450m, 0.616m)&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;a href=&quot;#Park2019&quot;&gt;(Park, Kim, Cho, &amp;amp; Kang, 2019)&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt;Fusion de données obtenues par différentes plateformes et différents capteurs (UAV et UGV)&lt;/td&gt;
			&lt;td&gt;Quadrotor et UGV&lt;/td&gt;
			&lt;td&gt;lidar (UGV)&lt;/td&gt;
			&lt;td&gt;Caméra (UAV)&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;O&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;-&lt;/td&gt;
			&lt;td&gt;Extérieur&lt;/td&gt;
			&lt;td&gt;ICP pour traiter les données, SLAM pour UGV, élimination des points redondants puis ICP, le drone doit prendre des images entre 30 degré et 90 degré pour optimiser les résultats&lt;/td&gt;
		&lt;/tr&gt;
	&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h1 id=&quot;références&quot;&gt;Références&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot; reversed=&quot;reversed&quot;&gt;&lt;li&gt;&lt;span id=&quot;Polewski2019&quot;&gt;Polewski, P., Yao, W., Cao, L., &amp;amp; Gao, S. (2019). Marker-free coregistration of UAV and backpack LiDAR point clouds in forested areas. &lt;i&gt;ISPRS Journal of Photogrammetry and Remote Sensing&lt;/i&gt;, &lt;i&gt;147&lt;/i&gt;(July 2018), 307–318. https://doi.org/10.1016/j.isprsjprs.2018.11.020&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Polewski2019/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Papachristos2019&quot;&gt;Papachristos, C., Khattak, S., Mascarich, F., &amp;amp; Alexis, K. (2019). &lt;i&gt;Autonomous Navigation and Mapping in Underground Mines Using Aerial Robots&lt;/i&gt;.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Papachristos2019/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Park2019&quot;&gt;Park, J., Kim, P., Cho, Y. K., &amp;amp; Kang, J. (2019). Framework for automated registration of UAV and UGV point clouds using local features in images. &lt;i&gt;Automation in Construction&lt;/i&gt;, &lt;i&gt;98&lt;/i&gt;(November 2018), 175–182. https://doi.org/10.1016/j.autcon.2018.11.024&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Park2019/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Cramer2018&quot;&gt;Cramer, M., Haala, N., Laupheimer, D., Mandlburger, G., &amp;amp; Havel, P. (2018). Ultra-High Precision Uav-Based Lidar and Dense Image Matching, &lt;i&gt;XLII&lt;/i&gt;(October), 10–12.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Cramer2018/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Gomes2018&quot;&gt;Gomes, A., Guerreiro, B. J., Cunha, R., Silvestre, C., &amp;amp; Oliveira, P. (2018). Sensor-based 3-D pose estimation and control of rotary-wing UAVs using a 2-D LiDAR. &lt;i&gt;Advances in Intelligent Systems and Computing&lt;/i&gt;, &lt;i&gt;693&lt;/i&gt;, 718–729. https://doi.org/10.1007/978-3-319-70833-1_58&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Gomes2018/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Hinterhofer2018&quot;&gt;Hinterhofer, T., Ullrich, A., Hofstätter, M., Pfennigbauer, M., Schraml, S., &amp;amp; Rothbacher, D. (2018). UAV-based LiDAR and gamma probe with real-time data processing and downlink for survey of nuclear disaster locations. &lt;i&gt;Chemical, Biological, Radiological, Nuclear, and Explosives (CBRNE) Sensing XIX&lt;/i&gt;, (May 2018), 11. https://doi.org/10.1117/12.2304353&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Hinterhofer2018/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Mastrangelo2018&quot;&gt;Mastrangelo, J., von Niederhausern, K., Nelson, R. D., &amp;amp; Fuller, D. (2018). Real-time LIDAR from ScanEagle UAV. &lt;i&gt;Ground/Air Multisensor Interoperability, Integration, and Networking for Persistent ISR IX&lt;/i&gt;, (May 2018), 33. https://doi.org/10.1117/12.2304393&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Mastrangelo2018/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Fuad2018&quot;&gt;Fuad, N. A., Ismail, Z., Majid, Z., Darwin, N., Ariff, M. F. M., Idris, K. M., &amp;amp; Yusoff, A. R. (2018). Accuracy evaluation of digital terrain model based on different flying altitudes and conditional of terrain using UAV LiDAR technology. &lt;i&gt;IOP Conference Series: Earth and Environmental Science&lt;/i&gt;, &lt;i&gt;169&lt;/i&gt;(1). https://doi.org/10.1088/1755-1315/169/1/012100&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Fuad2018/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Youn2018&quot;&gt;Youn, J., Kim, D., Kim, T., Yoo, J. H., &amp;amp; Lee, B. J. (2018). Development of Uav Air Roads By Using 3D Grid System. &lt;i&gt;ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences&lt;/i&gt;, &lt;i&gt;XLII-4&lt;/i&gt;(October), 731–735. https://doi.org/10.5194/isprs-archives-XLII-4-731-2018&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Youn2018/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Liu2018&quot;&gt;Liu, K., Shen, X., Cao, L., Wang, G., &amp;amp; Cao, F. (2018). Estimating forest structural attributes using UAV-LiDAR data in Ginkgo plantations. &lt;i&gt;ISPRS Journal of Photogrammetry and Remote Sensing&lt;/i&gt;, &lt;i&gt;146&lt;/i&gt;(November), 465–482. https://doi.org/10.1016/j.isprsjprs.2018.11.001&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Liu2018/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Huang2018&quot;&gt;Huang, Z. C., Yeh, C. Y., Tseng, K. H., &amp;amp; Hsu, W. Y. (2018). A UAV-RTK lidar system for wave and tide measurements in coastal zones. &lt;i&gt;Journal of Atmospheric and Oceanic Technology&lt;/i&gt;, &lt;i&gt;35&lt;/i&gt;(8), 1557–1570. https://doi.org/10.1175/JTECH-D-17-0199.1&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Huang2018/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Yan2018&quot;&gt;Yan, W., Guan, H., Cao, L., Yu, Y., Gao, S., Lu, J. Y., … Lu, J. Y. (2018). An Automated Hierarchical Approach for Three-Dimensional Segmentation of Single Trees Using UAV LiDAR Data. &lt;i&gt;Remote Sensing&lt;/i&gt;, &lt;i&gt;10&lt;/i&gt;(12), 1999. https://doi.org/10.3390/rs10121999&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Yan2018/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Chen2018&quot;&gt;Chen, C., Yang, B., Song, S., Peng, X., &amp;amp; Huang, R. (2018). Automatic clearance anomaly detection for transmission line corridors utilizing UAV-Borne LIDAR data. &lt;i&gt;Remote Sensing&lt;/i&gt;, &lt;i&gt;10&lt;/i&gt;(4). https://doi.org/10.3390/rs10040613&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Chen2018/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Govedarica2018&quot;&gt;Govedarica, M., Jakovljevic, G., &amp;amp; Álvarez Taboada, F. (2018). Flood risk assessment based on LiDAR and UAV points clouds and DEM. &lt;i&gt;Remote Sensing for Agriculture, Ecosystems, and Hydrology XX&lt;/i&gt;, (October 2018), 102. https://doi.org/10.1117/12.2513278&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Govedarica2018/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Torresan2018&quot;&gt;Torresan, C., Berton, A., Carotenuto, F., Chiavetta, U., Miglietta, F., Zaldei, A., &amp;amp; Gioli, B. (2018). Development and performance assessment of a low-cost UAV laser scanner system (LasUAV). &lt;i&gt;Remote Sensing&lt;/i&gt;, &lt;i&gt;10&lt;/i&gt;(7), 1–17. https://doi.org/10.3390/rs10071094&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Torresan2018/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Mascarich2018&quot;&gt;Mascarich, F., Wilson, T., Khattak, S., Dang, T., Papachristos, C., &amp;amp; Alexis, K. (2018). WM2018 Conference, March 18 – 22, 2018, Phoenix, Arizona, USA Autonomous 3D and Radiation Mapping in Tunnel Environments Using Aerial Robots – 18156 Frank Mascarich, Taylor Wilson, Shehryar Khattak, Tung Dang, Christos Papachristos, Kostas Alexis Universi, 1–12.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Mascarich2018/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Nasrollahi2018&quot;&gt;Nasrollahi, M., Bolourian, N., Zhu, Z., &amp;amp; Hammad, A. (2018). Designing LiDAR-equipped UAV Platform for Structural Inspection. &lt;i&gt;Proceedings of the 35th International Symposium on Automation and Robotics in Construction (ISARC)&lt;/i&gt;, (July). https://doi.org/10.22260/isarc2018/0152&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Nasrollahi2018/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Teng2017&quot;&gt;Teng, G. E., Zhou, M., Li, C. R., Wu, H. H., Li, W., Meng, F. R., … Ma, L. (2017). Mini-UAV LIDAR for power line inspection. &lt;i&gt;International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives&lt;/i&gt;, &lt;i&gt;42&lt;/i&gt;(2W7), 297–300. https://doi.org/10.5194/isprs-archives-XLII-2-W7-297-2017&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Teng2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Morsdorf2017&quot;&gt;Morsdorf, F., Eck, C., Zgraggen, C., Imbach, B., Schneider, F. D., &amp;amp; Kükenbrink, D. (2017). UAV-based LiDAR acquisition for the derivation of high-resolution forest and ground information. &lt;i&gt;The Leading Edge&lt;/i&gt;, &lt;i&gt;36&lt;/i&gt;(7), 566–570. https://doi.org/10.1190/tle36070566.1&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Morsdorf2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Gawel2017&quot;&gt;Gawel, A., Dube, R., Surmann, H., Nieto, J., Siegwart, R., &amp;amp; Cadena, C. (2017). 3D registration of aerial and ground robots for disaster response: An evaluation of features, descriptors, and transformation estimation. &lt;i&gt;SSRR 2017 - 15th IEEE International Symposium on Safety, Security and Rescue Robotics, Conference&lt;/i&gt;, 27–34. https://doi.org/10.1109/SSRR.2017.8088136&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Gawel2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Gasparovic2017&quot;&gt;Gašparović, M., Seletković, A., Berta, A., &amp;amp; Balenović, I. (2017). The Evaluation of Photogrammetry-Based DSM from Low-Cost UAV by LiDAR-Based DSM. &lt;i&gt;South-East European Forestry&lt;/i&gt;, &lt;i&gt;8&lt;/i&gt;(2). https://doi.org/10.15177/seefor.17-16&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Gasparovic2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Christiansen2017&quot;&gt;Christiansen, M. P., Laursen, M. S., Jørgensen, R. N., Skovsen, S., &amp;amp; Gislum, R. (2017). Designing and testing a UAV mapping system for agricultural field surveying. &lt;i&gt;Sensors (Switzerland)&lt;/i&gt;, &lt;i&gt;17&lt;/i&gt;(12), 1–19. https://doi.org/10.3390/s17122703&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Christiansen2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;AjayKumar2017&quot;&gt;Ajay Kumar, G., Patil, A. K., Patil, R., Park, S. S., &amp;amp; Chai, Y. H. (2017). A LiDAR and IMU integrated indoor navigation system for UAVs and its application in real-time pipeline classification. &lt;i&gt;Sensors (Switzerland)&lt;/i&gt;, &lt;i&gt;17&lt;/i&gt;(6). https://doi.org/10.3390/s17061268&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/AjayKumar2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Sankey2017&quot;&gt;Sankey, T., Donager, J., McVay, J., &amp;amp; Sankey, J. B. (2017). UAV lidar and hyperspectral fusion for forest monitoring in the southwestern USA. &lt;i&gt;Remote Sensing of Environment&lt;/i&gt;, &lt;i&gt;195&lt;/i&gt;, 30–43. https://doi.org/10.1016/j.rse.2017.04.007&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Sankey2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Chen2017&quot;&gt;Chen, S., McDermid, G. J., Castilla, G., &amp;amp; Linke, J. (2017). Measuring vegetation height in linear disturbances in the boreal forest with UAV photogrammetry. &lt;i&gt;Remote Sensing&lt;/i&gt;, &lt;i&gt;9&lt;/i&gt;(12). https://doi.org/10.3390/rs9121257&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Chen2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Nikolov2017&quot;&gt;Nikolov, I., &amp;amp; Madsen, C. (2017). LiDAR-based 2D Localization and Mapping System using Elliptical Distance Correction Models for UAV Wind Turbine Blade Inspection. &lt;i&gt;Proceedings of the 12th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications&lt;/i&gt;, (Visigrapp), 418–425. https://doi.org/10.5220/0006124304180425&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Nikolov2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Kwon2017&quot;&gt;Kwon, S., Park, J. W., Moon, D., Jung, S., &amp;amp; Park, H. (2017). Smart Merging Method for Hybrid Point Cloud Data using UAV and LIDAR in Earthwork Construction. &lt;i&gt;Procedia Engineering&lt;/i&gt;, &lt;i&gt;196&lt;/i&gt;(June), 21–28. https://doi.org/10.1016/j.proeng.2017.07.168&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Kwon2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Elaksher2017&quot;&gt;Elaksher, A., Bhandari, S., Carreon Limones, C. A., &amp;amp; Lauf, R. (2017). Potential of UAV lidar systems for geospatial mapping. &lt;i&gt;Lidar Remote Sensing for Environmental Monitoring 2017&lt;/i&gt;, (August 2017), 20. https://doi.org/10.1117/12.2275482&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Elaksher2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;PututAshShidiq2017&quot;&gt;Putut Ash Shidiq, I., Wibowo, A., Kusratmoko, E., Indratmoko, S., Ardhianto, R., &amp;amp; Prasetyo Nugroho, B. (2017). Urban forest topographical mapping using UAV LIDAR. &lt;i&gt;IOP Conference Series: Earth and Environmental Science&lt;/i&gt;, &lt;i&gt;98&lt;/i&gt;(1). https://doi.org/10.1088/1755-1315/98/1/012034&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/PututAshShidiq2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Guo2017&quot;&gt;Guo, Q., Su, Y., Hu, T., Zhao, X., Wu, F., Li, Y., … Wang, X. (2017). An integrated UAV-borne lidar system for 3D habitat mapping in three forest ecosystems across China. &lt;i&gt;International Journal of Remote Sensing&lt;/i&gt;, &lt;i&gt;38&lt;/i&gt;(8-10), 2954–2972. https://doi.org/10.1080/01431161.2017.1285083&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Guo2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Wei2017&quot;&gt;Wei, L., Yang, B., Jiang, J., Cao, G., &amp;amp; Wu, M. (2017). Vegetation filtering algorithm for UAV-borne lidar point clouds: a case study in the middle-lower Yangtze River riparian zone. &lt;i&gt;International Journal of Remote Sensing&lt;/i&gt;, &lt;i&gt;38&lt;/i&gt;(8-10), 2991–3002. https://doi.org/10.1080/01431161.2016.1252476&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Wei2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Thiel2017&quot;&gt;Thiel, C., &amp;amp; Schmullius, C. (2017). Comparison of UAV photograph-based and airborne lidar-based point clouds over forest from a forestry application perspective. &lt;i&gt;International Journal of Remote Sensing&lt;/i&gt;, &lt;i&gt;38&lt;/i&gt;(8-10), 2411–2426. https://doi.org/10.1080/01431161.2016.1225181&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Thiel2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Gee2017&quot;&gt;Gee, T., James, J., Van Der Mark, W., Delmas, P., &amp;amp; Gimel’Farb, G. (2017). Lidar guided stereo simultaneous localization and mapping (SLAM) for UAV outdoor 3-D scene reconstruction. &lt;i&gt;International Conference Image and Vision Computing New Zealand&lt;/i&gt;, 1–6. https://doi.org/10.1109/IVCNZ.2016.7804433&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Gee2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Persad2017&quot;&gt;Persad, R. A., &amp;amp; Armenakis, C. (2017). Comparison of 2d and 3D approaches for the alignment of UAV and lidar point clouds. &lt;i&gt;International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives&lt;/i&gt;, &lt;i&gt;42&lt;/i&gt;(2W6), 275–279. https://doi.org/10.5194/isprs-archives-XLII-2-W6-275-2017&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Persad2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Chiang2017&quot;&gt;Chiang, K. W., Tsai, G. J., Li, Y. H., &amp;amp; El-Sheimy, N. (2017). Development of LiDAR-Based UAV System for Environment Reconstruction. &lt;i&gt;IEEE Geoscience and Remote Sensing Letters&lt;/i&gt;, &lt;i&gt;14&lt;/i&gt;(10), 1790–1794. https://doi.org/10.1109/LGRS.2017.2736013&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Chiang2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Moore2017&quot;&gt;Moore, A. J., Schubert, M., Rymer, N., Balachandran, S., Consiglio, M., Munoz, C., … Schneider Georgia Power, P. (2017). UAV Inspection of Electrical Transmission Infrastructure with Path Conformance Autonomy and Lidar-based Geofences NASA Report on UTM Reference Mission Flights at Southern Company Flights November 2016 NASA STI Program . . . in Profile.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Moore2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Edwards2017&quot;&gt;Edwards, A. S. (2017). Autonomous 3D Mapping and Surveillance of Mines with MAVs, (March).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Edwards2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Shetty2017&quot;&gt;Shetty, A., &amp;amp; Gao, G. X. (2017). Covariance Estimation for GPS-LiDAR Sensor Fusion for UAVs, 1–5.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Shetty2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Koska2017&quot;&gt;Koska, B., Jirka, V., Urban, R., Křemen, T., Hesslerová, P., Jon, J., … Fogl, M. (2017). Suitability, characteristics, and comparison of an airship UAV with lidar for middle size area mapping. &lt;i&gt;International Journal of Remote Sensing&lt;/i&gt;, &lt;i&gt;38&lt;/i&gt;(8-10), 2973–2990. https://doi.org/10.1080/01431161.2017.1285086&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Koska2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Vempati2017&quot;&gt;Vempati, A. S., Gilitschenski, I., Nieto, J., Beardsley, P., &amp;amp; Siegwart, R. (2017). Onboard real-time dense reconstruction of large-scale environments for UAV. &lt;i&gt;IEEE International Conference on Intelligent Robots and Systems&lt;/i&gt;, &lt;i&gt;2017-September&lt;/i&gt;, 3479–3486. https://doi.org/10.1109/IROS.2017.8206189&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Vempati2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Ozaslan2017&quot;&gt;Ozaslan, T., Taylor, C. J., Kumar, V., Keller, J., Wozencraft, J. M., Loianno, G., &amp;amp; Hood, T. (2017). Autonomous Navigation and Mapping for Inspection of Penstocks and Tunnels With MAVs. &lt;i&gt;IEEE Robotics and Automation Letters&lt;/i&gt;, &lt;i&gt;2&lt;/i&gt;(3), 1740–1747. https://doi.org/10.1109/lra.2017.2699790&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Ozaslan2017/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Petras2016&quot;&gt;Petras, V., Petrasova, A., Jeziorska, J., &amp;amp; Mitasova, H. (2016). Processing UAV and LiDAR point clouds in grass GIS. &lt;i&gt;International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives&lt;/i&gt;, &lt;i&gt;41&lt;/i&gt;(July), 945–952. https://doi.org/10.5194/isprsarchives-XLI-B7-945-2016&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Petras2016/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Kasturi2016&quot;&gt;Kasturi, A., Milanovic, V., Atwood, B. H., &amp;amp; Yang, J. (2016). UAV-borne lidar with MEMS mirror-based scanning capability, (May 2016), 98320M. https://doi.org/10.1117/12.2224285&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Kasturi2016/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Reiss2016&quot;&gt;Reiss, M. L. L., Da Rocha, R. S., Ferraz, R. S., Cruz, V. C., Morador, L. Q., Yamawaki, M. K., … Mezzomo, W. (2016). Data integration acquired from micro-UAV and terrestrial laser scanner for the 3D mapping of jesuit ruins of são miguel das missões. &lt;i&gt;International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives&lt;/i&gt;, &lt;i&gt;41&lt;/i&gt;(July), 315–321. https://doi.org/10.5194/isprsarchives-XLI-B5-315-2016&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Reiss2016/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Lawson2015&quot;&gt;Lawson, A. (2015). Cooperative 3-D Map Generation Using Multiple UAVs. &lt;i&gt;University Scholar Projects&lt;/i&gt;.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Lawson2015/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Liu2015&quot;&gt;Liu, S., Mohta, K., Shen, S., &amp;amp; Kumar, V. (2015). Towards Collaborative Mapping and Exploration Using Multiple Micro Aerial Robots BT - Experimental Robotics: The 14th International Symposium on Experimental Robotics. &lt;i&gt;Experimental Robotics III&lt;/i&gt;. https://doi.org/10.1007/bfb0027579&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Liu2015/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Pan2015&quot;&gt;Pan, W. W., Dou, Y. J., Wang, G. L., Wu, M. X., Ren, R. G., &amp;amp; Xu, X. (2015). Development and test of blimp-based compact LIDAR powewr-line inspection system. &lt;i&gt;International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives&lt;/i&gt;, &lt;i&gt;40&lt;/i&gt;(3W2), 155–159. https://doi.org/10.5194/isprsarchives-XL-3-W2-155-2015&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Pan2015/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Li2015&quot;&gt;Li, Z., Yan, Y., Jing, Y., &amp;amp; Zhao, S. G. (2015). The design and testing of a LiDAR platform for a UAV for heritage mapping. &lt;i&gt;International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives&lt;/i&gt;, &lt;i&gt;40&lt;/i&gt;(1W4), 17–24. https://doi.org/10.5194/isprsarchives-XL-1-W4-17-2015&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Li2015/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Yousif2015&quot;&gt;Yousif, K., Bab-Hadiashar, A., &amp;amp; Hoseinnezhad, R. (2015). An Overview to Visual Odometry and Visual SLAM: Applications to Mobile Robotics. &lt;i&gt;Intelligent Industrial Systems&lt;/i&gt;, &lt;i&gt;1&lt;/i&gt;(4), 289–311. https://doi.org/10.1007/s40903-015-0032-7&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Yousif2015/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Gu2015&quot;&gt;Gu, T., &amp;amp; Zhang, N. (2015). Application of iterative closest point algorithm in automatic flight of speedy UAV. &lt;i&gt;2014 IEEE Chinese Guidance, Navigation and Control Conference, CGNCC 2014&lt;/i&gt;, 1456–1459. https://doi.org/10.1109/CGNCC.2014.7007407&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Gu2015/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Mandlburger2015&quot;&gt;Mandlburger, G., Pfennigbauer, M., Riegl, U., Haring, A., Wieser, M., Glira, P., &amp;amp; Winiwarter, L. (2015). Complementing airborne laser bathymetry with UAV-based lidar for capturing alluvial landscapes, (October 2015), 96370A. https://doi.org/10.1117/12.2194779&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Mandlburger2015/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Tulldahl2015&quot;&gt;Tulldahl, H. M., Bissmarck, F., Larsson, H., Grönwall, C., &amp;amp; Tolt, G. (2015). Accuracy evaluation of 3D lidar data from small UAV, &lt;i&gt;964903&lt;/i&gt;(October 2015), 964903. https://doi.org/10.1117/12.2194508&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Tulldahl2015/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Yang2015&quot;&gt;Yang, B., &amp;amp; Chen, C. (2015). Automatic registration of UAV-borne sequent images and LiDAR data. &lt;i&gt;ISPRS Journal of Photogrammetry and Remote Sensing&lt;/i&gt;, &lt;i&gt;101&lt;/i&gt;, 262–274. https://doi.org/10.1016/j.isprsjprs.2014.12.025&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Yang2015/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Ni2015&quot;&gt;Ni, W., Liu, J., Zhang, Z., Sun, G., &amp;amp; Yang, A. (2015). Evaluation of UAV-based forest inventory system compared with LiDAR data. &lt;i&gt;International Geoscience and Remote Sensing Symposium (IGARSS)&lt;/i&gt;, &lt;i&gt;2015-Novem&lt;/i&gt;, 3874–3877. https://doi.org/10.1109/IGARSS.2015.7326670&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Ni2015/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Esposito2014&quot;&gt;Esposito, S., Mura, M., Fallavollita, P., Balsi, M., Chirici, G., Oradini, A., &amp;amp; Marchetti, M. (2014). Performance evaluation of lightweight LiDAR for UAV applications. &lt;i&gt;International Geoscience and Remote Sensing Symposium (IGARSS)&lt;/i&gt;, 792–795. https://doi.org/10.1109/IGARSS.2014.6946543&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Esposito2014/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Roca2014&quot;&gt;Roca, D., Armesto, J., Lagüela, S., &amp;amp; Díaz-Vilariño, L. (2014). LIDAR-equipped UAV for building information modelling. &lt;i&gt;International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives&lt;/i&gt;, &lt;i&gt;40&lt;/i&gt;(5), 523–527. https://doi.org/10.5194/isprsarchives-XL-5-523-2014&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roca2014/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Tulldahl2014&quot;&gt;Tulldahl, H. M., &amp;amp; Larsson, H. (2014). Lidar on small UAV for 3D mapping, &lt;i&gt;925009&lt;/i&gt;(October 2014), 925009. https://doi.org/10.1117/12.2068448&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Tulldahl2014/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Wallace2014&quot;&gt;Wallace, L., Lucieer, A., &amp;amp; Watson, C. S. (2014). Evaluating tree detection and segmentation routines on very high resolution UAV LiDAR ata. &lt;i&gt;IEEE Transactions on Geoscience and Remote Sensing&lt;/i&gt;, &lt;i&gt;52&lt;/i&gt;(12), 7619–7628. https://doi.org/10.1109/TGRS.2014.2315649&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Wallace2014/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Lin2014&quot;&gt;Lin, Y., &amp;amp; West, G. (2014). Attempt of UAV oblique images and MLS point clouds for 4D modelling of roadside pole-like objects, &lt;i&gt;9262&lt;/i&gt;(November 2014), 92620Q. https://doi.org/10.1117/12.2068287&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Lin2014/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Zhou2013&quot;&gt;Zhou, G., Yang, B., Zhang, W., Tao, X., Zhao, W., Yue, T., … Yang, C. (2013). Simulation study of new generation of airborne scannerless LiDAR system. &lt;i&gt;International Geoscience and Remote Sensing Symposium (IGARSS)&lt;/i&gt;, (February 2015), 524–527. https://doi.org/10.1109/IGARSS.2013.6721208&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Zhou2013/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Wallace2013&quot;&gt;Wallace, L. (2013). Assessing the stability of canopy maps produced from UAV-LiDAR data. &lt;i&gt;International Geoscience and Remote Sensing Symposium (IGARSS)&lt;/i&gt;, (Figure 1), 3879–3882. https://doi.org/10.1109/IGARSS.2013.6723679&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Wallace2013/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Wallace2012a&quot;&gt;Wallace, L. O., Lucieer, A., &amp;amp; Watson, C. S. (2012). Assessing the Feasibility of Uav-Based Lidar for High Resolution Forest Change Detection. &lt;i&gt;ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences&lt;/i&gt;, &lt;i&gt;XXXIX-B7&lt;/i&gt;(September), 499–504. https://doi.org/10.5194/isprsarchives-XXXIX-B7-499-2012&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Wallace2012a/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Wallace2012&quot;&gt;Wallace, L., Lucieer, A., Watson, C., &amp;amp; Turner, D. (2012). Development of a UAV-LiDAR system with application to forest inventory. &lt;i&gt;Remote Sensing&lt;/i&gt;, &lt;i&gt;4&lt;/i&gt;(6), 1519–1543. https://doi.org/10.3390/rs4061519&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Wallace2012/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Michael2012&quot;&gt;Michael, N., Shen, S., Motha, K., Mulgaonkar, Y., Kumar, V., Nagatani, K., … Tadokoro, S. (2012). Collaborative mapping of an earthquake‐damaged building via ground and aerial robots. &lt;i&gt;\Ldots of Field Robotics&lt;/i&gt;, &lt;i&gt;29&lt;/i&gt;(5), 832–841. https://doi.org/10.1002/rob&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Michael2012/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Hwang2012&quot;&gt;Hwang, Y., &amp;amp; Tahk, M.-jea. (2012). Terrain Referenced UAV Navigation with Lidar – a Comparison of Sequential Processing and Batch Processing Algorithms. &lt;i&gt;28th INTERNATIONAL CONGRESS OF THE AERONAUTICAL SCIENCES&lt;/i&gt;, 1–7.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Hwang2012/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Candigliota2012&quot;&gt;Candigliota, E., Immordino, F., Moretti, L., &amp;amp; Indirli, M. (2012). Remote sensing, laser scanner survey and GIS integrated method for assessment and preservation of historic centers: the example of Arsita. &lt;i&gt;In Proceedings of the 15th World Conference on Earthquake Engineering - WCEE&lt;/i&gt;, 1–9.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Candigliota2012/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Han2012&quot;&gt;Han, K., Aeschliman, C., Park, J., Kak, A. C., Kwon, H., &amp;amp; Pack, D. J. (2012). UAV vision: Feature based accurate ground target localization through propagated initializations and interframe homographies. &lt;i&gt;Proceedings - IEEE International Conference on Robotics and Automation&lt;/i&gt;, 944–950. https://doi.org/10.1109/ICRA.2012.6225073&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Han2012/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Durst2011&quot;&gt;Durst, P. J., Baylot, A., &amp;amp; McKinley, B. (2011). Techniques for inferring terrain parameters related to ground vehicle mobility using UAV born IFSAR and LIDAR data. &lt;i&gt;Proceedings of SPIE - The International Society for Optical Engineering&lt;/i&gt;, &lt;i&gt;8020&lt;/i&gt;(May 2011). https://doi.org/10.1117/12.883510&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Durst2011/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Liu2011&quot;&gt;Liu, C., Li, W., Lei, W., Liu, L., &amp;amp; Wu, H. (2011). Architecture planning and geo-disasters assessment mapping of landslide by using airborne lidar data and UAV images, (October 2011), 82861Q. https://doi.org/10.1117/12.912525&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Liu2011/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Downs2004&quot;&gt;Downs, A., Madhavan, R., &amp;amp; Hong, T. (2004). Registration of range data from unmanned aerial and ground vehicles. &lt;i&gt;Proceedings - Applied Imagery Pattern Recognition Workshop&lt;/i&gt;, &lt;i&gt;2003-Janua&lt;/i&gt;, 45–50. https://doi.org/10.1109/AIPR.2003.1284247&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Downs2004/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Zhen&quot;&gt;Zhen, W., &amp;amp; Scherer, S. A Unified 3D Mapping Framework using a 3D or 2D LiDAR. &lt;i&gt;Robotics&lt;/i&gt;, 1–10.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;/bibliography/Zhen/&quot;&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Maxime Vaidis</name><email>vaidis.maxime@gmail.com</email></author><category term="publications" /><category term="ICP" /><category term="registration" /><category term="mapping" /><category term="robot" /><category term="localization" /><summary type="html">Cette enquête présente une vue générale des projets utilisant des lidars (Technologie de mesure de distance et détection par laser) sur des UAV (Véhicule Aérien Autonome).</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22feature%22=%3E%22publications/survey-lidar-UAV_feature.jpg%22,%20%22teaser%22=%3E%22publications/survey-lidar-UAV_teaser.jpg%22,%20%22thumb%22=%3Enil,%20%22credit%22=%3E%22H2H%20Associates,%20DroneZon%22%7D" /></entry></feed>