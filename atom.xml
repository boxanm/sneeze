<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="https://norlab.ulaval.ca/atom.xml" rel="self" type="application/atom+xml" /><link href="https://norlab.ulaval.ca/" rel="alternate" type="text/html" /><updated>2020-05-07T15:53:03-04:00</updated><id>https://norlab.ulaval.ca/atom.xml</id><title type="html">Northern Robotics Laboratory</title><subtitle>Website showcasing research and news from the Northern Robotics Laboratory, Laval University</subtitle><entry><title type="html">Project SNOW</title><link href="https://norlab.ulaval.ca/research/snow/" rel="alternate" type="text/html" title="Project SNOW" /><published>2020-04-25T00:00:00-04:00</published><updated>2020-04-25T00:00:00-04:00</updated><id>https://norlab.ulaval.ca/research/snow</id><content type="html" xml:base="https://norlab.ulaval.ca/research/snow/">&lt;p&gt;Self-driving cars are expected on our roads soon. In the project SNOW (Self-driving Navigation Optimized for Winter), we focus on the unexplored problem of autonomous driving during winter that still raises reliability concerns. We have the expertise to automatically build 3D maps of the environment while moving through it with robots. We aim at using this knowledge to investigate mapping and control solutions for challenging conditions related to Canadian weather.&lt;/p&gt;

&lt;p&gt;The main goal of this project is to extend the current technology used for autonomous driving toward unstructured and dynamic environments generated by winter conditions (e.g., a snow-covered forest). This project is addressing the applications of autonomous driving in remote areas, autonomous refueling, search and rescue missions, Canadian Arctic Sovereignty, freight transport on Northern ice roads, etc. Our research concentrates on maps built by a UGV, which will be able to adapt to environmental changes caused by snow and winds, while allowing a robust localization of the vehicle in real-time. These maps will also serve as the foundation for novel path planning algorithms handling deformable obstacles and environments (e.g., deep snow under a vehicle). The project is carried out in partnership with &lt;a href=&quot;https://www.gdlscanada.com/&quot; title=&quot;Official website&quot;&gt;General Dynamics Land Systems - Canada (GDLS-C)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To achieve the goals of the project, the following specific objectives are defined:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Objective 1—Mapping and Localization:&lt;/strong&gt; to develop algorithms to allow the UGV to localize and
map its environment in winter conditions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Objective 2—Path Planning and Control:&lt;/strong&gt; to develop algorithms for planning paths and adapt
the behavior of the UGV according to weather conditions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Objective 3—Field Testing and Integration:&lt;/strong&gt; to carry out an extensive series of experiments using
the UGV in a snow-covered forest.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;from-theory-to-practice&quot;&gt;From theory to practice&lt;/h2&gt;
&lt;p&gt;The following video shows the day we received the UGV - a Warthog robot made by a Canadian company, Clearpath Robotics. We have received the robot in August 2019 and since we began to realize ours plans in practice: 
&lt;!-- blank line --&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/bmGcV2TA0dY&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;integration-of-the-ugv-hardware-and-software&quot;&gt;Integration of the UGV hardware and software&lt;/h2&gt;
&lt;p&gt;Since the reception of the UGV, we have had a detailed plan of the tasks to follow to meet our goals, one of them being able to test the robot in a snowy forest during the winter of 2019. The first step was to integrate the sensor suite we had planned for the UGV.  This also involved constructing a solid metal frame by ours hands.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/images/projects/snow/forest_3.jpg&quot; style=&quot;width: 80%&quot; /&gt;&lt;/center&gt;

&lt;p&gt;When we finished this electrical part, we continued with the software integration. The mapping software required integration with the system of the UGV, mainly communication with its low-level computer which provides time synchronization and wheel odometry. After finishing all these tasks, it was finally the time to start testing. We found a nice and practical place for initial testing in front of our faculty building. In September 2019 when we fixed all the important details, we were able to proceed to the &lt;a href=&quot;https://www.foretmontmorency.ca/en/&quot; title=&quot;Official website&quot;&gt;Montmorency forest&lt;/a&gt;.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/images/projects/snow/forest_1.jpg&quot; style=&quot;width: 80%&quot; /&gt;&lt;/center&gt;

&lt;p&gt;The main goal of the first tests in the forest was to prove that we could safely deploy the robot, record all necessary data and possibly run mapping on-board the robot computers. We have, of course, discovered some bugs but they were all quick to fix and after a few sessions, we were able to generate large 3D maps of the forest.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/images/projects/snow/color_map.jpg&quot; style=&quot;width: 80%&quot; /&gt;&lt;/center&gt;

&lt;p&gt;Because the mapping functionality seemed working fine, we proceeded to implementing a basic path-following functionality. The robot would be navigated once by hand over a desired path while recording its position. Then, a software controller would repeat the path indefinitely, based on the onboard mapping and localization capabilities. The initial tuning of the controller showed some minor oscillations around the learned trajectory:&lt;/p&gt;

&lt;center&gt;
&lt;figure class=&quot;video_container&quot;&gt;
    &lt;video width=&quot;80%&quot; controls=&quot;&quot;&gt;
        &lt;source src=&quot;/images/projects/snow/oscillating.mp4&quot; type=&quot;video/mp4&quot; /&gt;
        Your browser does not support the video tag.
    &lt;/video&gt;
&lt;/figure&gt;
&lt;/center&gt;

&lt;p&gt;Fortunately, we were able to find the source of the problem, which was a misalignment between the commanded and actually executed turning rate of the robot. After fixing the problem, the controller was able to give us much more satisfying results:&lt;/p&gt;

&lt;center&gt;
&lt;figure class=&quot;video_container&quot;&gt;
    &lt;video muted=&quot;&quot; width=&quot;80%&quot; controls=&quot;&quot;&gt;
        &lt;source src=&quot;/images/projects/snow/better_control.mp4&quot; type=&quot;video/mp4&quot; /&gt;
        Your browser does not support the video tag.
    &lt;/video&gt;
&lt;/figure&gt;
&lt;/center&gt;

&lt;h2 id=&quot;first-review-meeting&quot;&gt;First review meeting&lt;/h2&gt;
&lt;p&gt;At the end of October 2019, the first review meeting took place. It lasted two days and was intended for our partner GDLS-C. We presented the integrated system and its capabilities to allow the partner to replicate the results on their identical UGV. The following video summarizes what was achieved:&lt;/p&gt;

&lt;!-- blank line --&gt;
&lt;figure class=&quot;video_container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/LILvVsYoaAg&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;team&quot;&gt;Team&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Principal Investigator and Technical Lead&lt;/strong&gt;: &lt;a href=&quot;../../people/f_pomerleau/&quot;&gt;François Pomerleau&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deputy Technical Lead&lt;/strong&gt;: &lt;a href=&quot;../../people/v_kubelka&quot;&gt;Vladimír Kubelka&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Representing GDLS-C&lt;/strong&gt;: &lt;a href=&quot;https://www.linkedin.com/in/richard-lee-gdls-ic/&quot;&gt;Richard Lee&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Master students&lt;/strong&gt;: &lt;a href=&quot;../../people/d_baril&quot;&gt;Dominic Baril&lt;/a&gt;, &lt;a href=&quot;../../people/&quot;&gt;Simon-Pierre Deschênes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jana Bartakova</name></author><category term="project" /><category term="snow" /><category term="robot" /><category term="warthog" /><summary type="html">Self-driving cars are expected on our roads soon. In the project SNOW (Self-driving Navigation Optimized for Winter), we focus on the unexplored problem of autonomous driving during winter that still raises reliability concerns. We have the expertise to automatically build 3D maps of the environment while moving through it with robots. We aim at using this knowledge to investigate mapping and control solutions for challenging conditions related to Canadian weather.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22feature%22=%3E%22/projects/snow/forest_2_crop.jpg%22,%20%22teaser%22=%3E%22/projects/snow/snow_teaser.jpg%22%7D" /></entry><entry><title type="html">DARPA - SUBTERRANEAN CHALLENGE</title><link href="https://norlab.ulaval.ca/research/darpa-subt-urban/" rel="alternate" type="text/html" title="DARPA - SUBTERRANEAN CHALLENGE" /><published>2020-04-14T00:00:00-04:00</published><updated>2020-04-14T00:00:00-04:00</updated><id>https://norlab.ulaval.ca/research/darpa-subt-urban</id><content type="html" xml:base="https://norlab.ulaval.ca/research/darpa-subt-urban/">&lt;p&gt;&lt;a href=&quot;https://www.subtchallenge.com&quot;&gt;DARPA - Subterranean Challenge (DARPA-SubT)&lt;/a&gt; is an international robotics competition focusing on autonomy, perception, networking, mobility technologies, and mapping underground areas in unpredictable conditions. This challenge connects the best researchers and teams to push the boundaries of what is possible in the field of mobile robotics. This competition consists of four circuits: tunnel systems, urban underground, cave networks and a combination of all of these together. It is also divided into the System track and the Virtual track and intended for both DARPA-funded and self-funded teams.&lt;/p&gt;

&lt;p&gt;Norlab competed in the System track of the urban circuit as a member of the CTU-CRAS-NORLAB team with the &lt;a href=&quot;https://robotics.fel.cvut.cz/cras&quot;&gt;Center for Robotics and Autonomous Systems (CRAS)&lt;/a&gt; from the Czech Technical University (CTU). Our researches could participate in the competition with a Husky robot thanks to generous funding by General Dynamics Land Systems – Canada. Together with the wheeled Husky robot, three tracked Absolem robots, three hexapod robots and two multi-rotor drones were deployed in the competition.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/images/projects/subt_urban/robots_entry.jpg&quot; style=&quot;width: 40%&quot; /&gt;&lt;/center&gt;

&lt;p&gt;The System track consisted of two courses, the Alpha Course and the Beta Course, each of them would be attempted twice. They were both situated in the underground of an &lt;a href=&quot;https://en.wikipedia.org/wiki/WNP-3_and_WNP-5&quot; title=&quot;Wikipedia&quot;&gt;unfinished nuclear plant&lt;/a&gt; close to Elma, Washington, in the United States. In every course, there were 20 hidden artifacts to be found by the robots in less than sixty minutes. The artifacts consisted of manikins (heated and wearing reflective jackets and hats), red backpacks, working cell phones (video, Wi-Fi, and Bluetooth), CO2 gas and air vents. The position of the artifacts would change between the two possible attempts. The goal was to detect and localize as many artifacts as possible.&lt;/p&gt;

&lt;p&gt;Our team deployed a combination of robots with different types of locomotion to be prepared for various obstacles expected in the man-made underground. The core of the robotic team consisted of three tracked robots that were able to navigate stairs and therefore explore multiple floors. Norlab provided a Husky robot allowing fast exploration on a single floor. Finally, drones and walking hexapods extended the reach of our team by either flying further (drones) or carrying radio modules (hexapods) extending the communication range. Our team chose lidars to be its primary mean of mapping and localization which also went well with the research focus of Norlab. Together with the thorough preparation of the artifact detectors on the Czech side and intensive testing, we were able to accurately localize artifacts the robots encountered. Motivated by the performance of the winning team, we now focus on improving the speed of exploration through better cooperation between the robots and higher autonomy.&lt;/p&gt;

&lt;h1 id=&quot;team&quot;&gt;Team&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Principal Investigator&lt;/strong&gt;: &lt;a href=&quot;http://cmp.felk.cvut.cz/~svoboda/&quot;&gt;Tomas Svoboda&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scientific Lead - 3D mapping&lt;/strong&gt;: &lt;a href=&quot;../../people/f_pomerleau/&quot;&gt;François Pomerleau&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Researchers - 3D mapping&lt;/strong&gt;: &lt;a href=&quot;../../people/v_kubelka&quot;&gt;Vladimír Kubelka&lt;/a&gt;, &lt;a href=&quot;../../people/m_vaidis&quot;&gt;Maxime Vaidis&lt;/a&gt;, &lt;a href=&quot;../../people/d_baril&quot;&gt;Dominic Baril&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://robotics.fel.cvut.cz/cras/darpa-subt/&quot;&gt;Team web page from CTU&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/subt_urban/robots_not_ready.jpg&quot; style=&quot;width: 100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;During this exciting competition, we also managed to make a few interesting stunts. One of them being the tracked Absolem robot running over and partially crushing the plastic hexapod robot. Another exciting stunt was overturning the Absolem robot on its back while climbing stairs. Also, we broke several drones, and in the end, one burned. Despite all these hardships, the team managed to score 10 points and took an amazing first place as a self-funded team and a third place among all teams.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/subt_urban/prize.jpg&quot; style=&quot;width: 100%&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;video&quot;&gt;Video&lt;/h1&gt;
&lt;!-- blank line --&gt;
&lt;figure class=&quot;video_container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/u_7DwnYTf-I&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/figure&gt;
&lt;!-- blank line --&gt;
&lt;figure class=&quot;video_container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/4hdeFdt0woc?start=14689&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/figure&gt;
&lt;!-- blank line --&gt;
&lt;figure class=&quot;video_container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/V9tzKF0hxnE?start=7475&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/figure&gt;
&lt;!-- blank line --&gt;
&lt;figure class=&quot;video_container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/O_c7P1TJKM4?start=15215&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/figure&gt;
&lt;!-- blank line --&gt;
&lt;figure class=&quot;video_container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/SgPirHYszzw?start=5997&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/figure&gt;
&lt;!-- blank line --&gt;
&lt;figure class=&quot;video_container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/rTP64z52JFE&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/figure&gt;
&lt;!-- blank line --&gt;
&lt;figure class=&quot;video_container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/3tqBO0HbLHY&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/figure&gt;
&lt;!-- blank line --&gt;

&lt;h1 id=&quot;media-coverage&quot;&gt;Media Coverage&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;IEEE SPECTRUM - &lt;a href=&quot;https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robotics-teams-prepare-darpa-subt-challenge-urban-circuit&quot;&gt;How Robotics Teams Prepared for DARPA’s SubT Challenge: Urban Circuit&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;IEEE SPECTRUM - &lt;a href=&quot;https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-quadruped-robot-locomotion-skills&quot;&gt;Video Friday: Quadruped Robot Learns Locomotion Skills by Imitating Dog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;THE ROBOTREPORT - &lt;a href=&quot;https://www.therobotreport.com/team-costar-wins-urban-circuit-of-darpa-subterranean-challenge/&quot;&gt;Team CoSTAR wins Urban Circuit of DARPA Subterranean Challenge&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;GEEKWIRE - &lt;a href=&quot;https://www.geekwire.com/2020/robots-masters-take-nuclear-plant-darpas-subterranean-challenge/&quot;&gt;Robots and their masters take over nuclear plant for DARPA’s Subterranean Challenge&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;ROS Discourse - &lt;a href=&quot;https://discourse.ros.org/t/subt-challenge-urban-circuit-overview/13635&quot;&gt;SubT Challenge Urban Circuit Overview&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The Vidette - &lt;a href=&quot;https://www.thevidette.com/news/robots-creeping-around-satsop-hoping-to-find-millions-in-prize-money/&quot;&gt;Robots creeping around Satsop hoping to find millions in prize money&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;ULaval (french) - &lt;a href=&quot;https://nouvelles.ulaval.ca/recherche/des-robots-dans-les-dedales-dune-centrale-nucleaire-abandonnee-a7a44f1b225efdc51eb5c4d657082e0a?sourceOrganizationKey=ulaval&quot;&gt;Des robots dans les dédales d’une centrale nucléaire abandonnée&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;ULaval (french) - &lt;a href=&quot;https://www.fsg.ulaval.ca/faculte/actualites/norlab-et-ses-partenaires-se-distinguent-au-darpa-subterranean-challenge-3400/&quot;&gt;Norlab et ses partenaires se distinguent au DARPA Subterranean Challenge&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;DARPA NEWS - &lt;a href=&quot;https://www.darpa.mil/news-events/2020-02-27&quot;&gt;Teams CoSTAR and BARCS Take Top Spots in DARPA Subterranean Challenge Urban Circuit&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;DARPA NEWS - &lt;a href=&quot;https://www.darpa.mil/news-events/2020-01-10&quot;&gt;DARPA Names Qualifiers for the Subterranean Challenge Urban Circuit&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;NEWSTROTTEUR (french) - &lt;a href=&quot;https://newstrotteur.fr/2020/02/28/des-robots-prennent-le-controle-dune-centrale-nucleaire-pour-le-defi-souterrain-de-la-darpa-newstrotteur/&quot;&gt;Des robots prennent le contrôle d’une centrale nucléaire pour le défi souterrain de la DARPA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;MAGNETPRESS (czech) - &lt;a href=&quot;https://www.vydavatelstvo-mps.sk/atm/6800-robotici-z-fel-cvut-uspeli-v-soutezi.html&quot;&gt;Robotici z FEL ČVUT uspěli v soutěži&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;CZECH WEBSITE: CTU NEWS (czech) - &lt;a href=&quot;https://aktualne.cvut.cz/stalo-se/20200228-robotici-z-fakulty-elektrotechnicke-vyhrali-svetovou-soutez-darpa-subterranean&quot;&gt;Robotici Z Fakulty Elektrotechnické Vyhráli Světovou Soutěž Darpa Subterranean Challenge Urban Circuit Mezi Nesponzorovanými Týmy A Obsadili Celkově 3. Místo&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;SCIENCEMAG (czech) - &lt;a href=&quot;https://sciencemag.cz/robotici-z-cvut-vyhrali-svetovou-soutez-darpa/&quot;&gt;Robotici z ČVUT vyhráli světovou soutěž DARPA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;DRONEWEB (czech) - &lt;a href=&quot;http://www.droneweb.cz/aktuality/item/345-drony-autonomni-fel-cvut-darpa-soutez&quot;&gt;České robotické drony budou soutěžit v americkém podzemí&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jana Bartakova</name></author><category term="project" /><category term="darpa" /><category term="competition" /><category term="subterranean" /><category term="robots" /><summary type="html">DARPA - Subterranean Challenge (DARPA-SubT) is an international robotics competition focusing on autonomy, perception, networking, mobility technologies, and mapping underground areas in unpredictable conditions. This challenge connects the best researchers and teams to push the boundaries of what is possible in the field of mobile robotics. This competition consists of four circuits: tunnel systems, urban underground, cave networks and a combination of all of these together. It is also divided into the System track and the Virtual track and intended for both DARPA-funded and self-funded teams.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22feature%22=%3E%22/projects/subt_urban/subt_main.jpg%22,%20%22teaser%22=%3E%22/projects/subt_urban/subt_teaser.jpg%22%7D" /></entry><entry><title type="html">The Montmorency dataset</title><link href="https://norlab.ulaval.ca/research/montmorencydataset/" rel="alternate" type="text/html" title="The Montmorency dataset" /><published>2020-01-25T00:00:00-05:00</published><updated>2020-01-25T00:00:00-05:00</updated><id>https://norlab.ulaval.ca/research/montmorencydataset</id><content type="html" xml:base="https://norlab.ulaval.ca/research/montmorencydataset/">&lt;p&gt;Under construction&lt;/p&gt;

&lt;p&gt;The dataset in now available for download on &lt;a href=&quot;https://academictorrents.com/details/cf39b5c4285f20c3539cbe9f37f6e04cfde10afa&quot;&gt;Academic Torrents&lt;/a&gt;.
This dataset contains the ground truth species, diameter at breast height and position of more than 1000 trees across four forests, as well as 11 trajectories of a lidar-equipped robot going through these forests.
It has been the subject of a publication at the 12th Conference on Field and Service Robotics.&lt;/p&gt;

&lt;p&gt;More details will appear later as this work is still undergoing peer-review.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot; reversed=&quot;reversed&quot;&gt;&lt;li&gt;&lt;span id=&quot;Tremblay2019&quot;&gt;Tremblay, J.-F., Béland, M., Pomerleau, F., Gagnon, R., &amp;amp; Giguère, P. (2019). Automatic 3D Mapping for Tree Diameter Measurements in Inventory Operations. In &lt;i&gt;Proceedings of the Conference on Field and Service Robotics (FSR). Springer Tracts in Advanced Robotics&lt;/i&gt;.&lt;/span&gt;


&lt;a href=&quot;/pdf/Tremblay2019.pdf&quot;&gt;&lt;i class=&quot;fas fa-file-pdf&quot;&gt;&lt;/i&gt; PDF&lt;/a&gt;


&lt;a href=&quot;/pdf/Tremblay2019.slides.pdf&quot;&gt;&lt;i class=&quot;fas fa-file-powerpoint&quot;&gt;&lt;/i&gt; Slides&lt;/a&gt;

&lt;a class=&quot;details&quot; href=&quot;/bibliography/Tremblay2019/&quot;&gt; &lt;i class=&quot;fas fa-file-code&quot;&gt;&lt;/i&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Jean-François Tremblay</name></author><category term="project" /><category term="mapping" /><category term="forestry" /><category term="lidar" /><summary type="html">Under construction</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22feature%22=%3E%22projects/forest_mapping_feature.jpg%22,%20%22teaser%22=%3E%22projects/forest_mapping_teaser.jpg%22,%20%22thumb%22=%3Enil%7D" /></entry><entry><title type="html">Our first group photo!</title><link href="https://norlab.ulaval.ca/news/christmas/" rel="alternate" type="text/html" title="Our first group photo!" /><published>2019-12-11T00:00:00-05:00</published><updated>2019-12-11T00:00:00-05:00</updated><id>https://norlab.ulaval.ca/news/christmas</id><content type="html" xml:base="https://norlab.ulaval.ca/news/christmas/">&lt;p&gt;It has been a good year in term of team building and equipment acquisition. 
Next year will most probably reserve us a lot of surprises, but I’m certain that we are ready for new scientific breakthroughs.&lt;/p&gt;</content><author><name>François Pomerleau</name><email>francois.pomerleau@ift.ulaval.ca</email></author><category term="photo" /><category term="group" /><summary type="html">It has been a good year in term of team building and equipment acquisition. Next year will most probably reserve us a lot of surprises, but I’m certain that we are ready for new scientific breakthroughs.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22feature%22=%3E%22/news/christmas19/christmas_feature.jpg%22,%20%22teaser%22=%3E%22/news/christmas19/christmas_teaser.jpg%22%7D" /></entry><entry><title type="html">Philippe Babin</title><link href="https://norlab.ulaval.ca/people/p_babin/" rel="alternate" type="text/html" title="Philippe Babin" /><published>2019-09-13T00:00:00-04:00</published><updated>2019-09-13T00:00:00-04:00</updated><id>https://norlab.ulaval.ca/people/p_babin</id><content type="html" xml:base="https://norlab.ulaval.ca/people/p_babin/">&lt;p&gt;Philippe Babin is currently employed at &lt;a href=&quot;https://argo.ai&quot;&gt;Argo AI&lt;/a&gt; a self-driving car startup.
He got a Bachelor’s Degree in Computer Science Engineering at Université Laval in 2017.
Throught his master, he works on improving the robustness of Iterative Closest Point (ICP).
His first project explored the effects of outlier filters (such as M-estimator) on ICP.
He then worked on the development of LIDAR SLAM algorithm for a forest mapping application.&lt;/p&gt;

&lt;h1 id=&quot;education&quot;&gt;Education&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;M.Sc. in Computer Science - Université Laval, 2017-2019&lt;/li&gt;
  &lt;li&gt;Bachelor’s Degree in Computer Science Engineering - Distinction Profile - Université Laval, 2013-2017&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;publications&quot;&gt;Publications&lt;/h1&gt;

&lt;h2 class=&quot;bibliography&quot;&gt;Conference Articles&lt;/h2&gt;
&lt;ol class=&quot;bibliography&quot; reversed=&quot;reversed&quot;&gt;&lt;li&gt;&lt;span id=&quot;Dandurand2019&quot;&gt;Dandurand, P., Babin, P., Kubelka, V., Giguère, P., &amp;amp; Pomerleau, F. (2019). Predicting GNSS satellite visibility from dense point clouds. In &lt;i&gt;Proceedings of the Conference on Field and Service Robotics (FSR). Springer Tracts in Advanced Robotics&lt;/i&gt;.&lt;/span&gt;


&lt;a href=&quot;/pdf/Dandurand2019.pdf&quot;&gt;&lt;i class=&quot;fas fa-file-pdf&quot;&gt;&lt;/i&gt; PDF&lt;/a&gt;


&lt;a class=&quot;details&quot; href=&quot;/bibliography/Dandurand2019/&quot;&gt; &lt;i class=&quot;fas fa-file-code&quot;&gt;&lt;/i&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Babin2019a&quot;&gt;Babin, P., Dandurand, P., Kubelka, V., Giguère, P., &amp;amp; Pomerleau, F. (2019). Large-scale 3D Mapping of Subarctic Forests. In &lt;i&gt;Proceedings of the Conference on Field and Service Robotics (FSR). Springer Tracts in Advanced Robotics&lt;/i&gt;.&lt;/span&gt;


&lt;a href=&quot;/pdf/Babin2019a.pdf&quot;&gt;&lt;i class=&quot;fas fa-file-pdf&quot;&gt;&lt;/i&gt; PDF&lt;/a&gt;


&lt;a href=&quot;/pdf/Babin2019a.slides.pdf&quot;&gt;&lt;i class=&quot;fas fa-file-powerpoint&quot;&gt;&lt;/i&gt; Slides&lt;/a&gt;

&lt;a class=&quot;details&quot; href=&quot;/bibliography/Babin2019a/&quot;&gt; &lt;i class=&quot;fas fa-file-code&quot;&gt;&lt;/i&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Babin2019&quot;&gt;Babin, P., Giguère, P., &amp;amp; Pomerleau, F. (2019). Analysis of Robust Functions for Registration Algorithms. In &lt;i&gt;Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)&lt;/i&gt;.&lt;/span&gt;


&lt;a href=&quot;/pdf/Babin2019.pdf&quot;&gt;&lt;i class=&quot;fas fa-file-pdf&quot;&gt;&lt;/i&gt; PDF&lt;/a&gt;


&lt;a class=&quot;details&quot; href=&quot;/bibliography/Babin2019/&quot;&gt; &lt;i class=&quot;fas fa-file-code&quot;&gt;&lt;/i&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
&lt;h2 class=&quot;bibliography&quot;&gt;Miscellaneous&lt;/h2&gt;
&lt;ol class=&quot;bibliography&quot; reversed=&quot;reversed&quot;&gt;&lt;li&gt;&lt;span id=&quot;Babin2018&quot;&gt;Babin, P., Pomerleau, F., &amp;amp; Giguère, P. (2018). Improving the robustness of registration algorithm in complex environments. Colloque du regroupement FRQNT-REPARTI.&lt;/span&gt;



&lt;a class=&quot;details&quot; href=&quot;/bibliography/Babin2018/&quot;&gt; &lt;i class=&quot;fas fa-file-code&quot;&gt;&lt;/i&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Philippe Babin</name></author><summary type="html">Philippe Babin is currently employed at Argo AI a self-driving car startup. He got a Bachelor’s Degree in Computer Science Engineering at Université Laval in 2017. Throught his master, he works on improving the robustness of Iterative Closest Point (ICP). His first project explored the effects of outlier filters (such as M-estimator) on ICP. He then worked on the development of LIDAR SLAM algorithm for a forest mapping application.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22feature%22=%3E%22/people/p_babin.jpg%22,%20%22teaser%22=%3E%22/people/p_babin_avatar.jpg%22%7D" /></entry><entry><title type="html">Large-scale 3D Mapping of Subarctic Forests</title><link href="https://norlab.ulaval.ca/publications/penality-icp/" rel="alternate" type="text/html" title="Large-scale 3D Mapping of Subarctic Forests" /><published>2019-09-13T00:00:00-04:00</published><updated>2019-09-13T00:00:00-04:00</updated><id>https://norlab.ulaval.ca/publications/penality-icp</id><content type="html" xml:base="https://norlab.ulaval.ca/publications/penality-icp/">&lt;p&gt;The ability to map challenging subarctic environments opens new horizons for robotic deployments in industries such as forestry, surveillance, and open-pit mining. In this paper, we explore possibilities of large-scale lidar mapping in a boreal forest. Computational and sensory requirements with regards to contemporary hardware are considered as well. The lidar mapping is often based on the SLAM technique relying on pose graph optimization, which fuses the Iterative Closest Point (ICP) algorithm, Global Navigation Satellite System (GNSS) positioning, and Inertial Measurement Unit (IMU) measurements.  To handle those sensors directly within the ICP minimization process, we propose an alternative approach of embedding external constraints. Furthermore, a novel formulation of a cost function is presented and cast into the problem of handling uncertainties from GNSS and lidar points. To test our approach, we acquired a large-scale dataset in the Foret Montmorency research forest. We report on the technical problems faced during our winter deployments aiming at building 3D maps using our new cost function. Those maps demonstrate both global and local consistency over 4.1km.&lt;/p&gt;

&lt;h1 id=&quot;quick-facts&quot;&gt;Quick facts:&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;4.1 km of untapped forest trail and snowmobile was mapped using a custom acquisition platform&lt;/li&gt;
  &lt;li&gt;The acquisition platform has a Xsens MTI-30 IMU, a &lt;a href=&quot;https://www.robosense.ai/rslidar/rs-lidar-16&quot;&gt;RoboSense RS-lidar-16&lt;/a&gt; and a REACH RS+ GNSS station&lt;/li&gt;
  &lt;li&gt;All of field test were done at &lt;a href=&quot;https://www.foretmontmorency.ca/en/&quot;&gt;Forêt Montmorency&lt;/a&gt;, a research forest owns by Université Laval.&lt;/li&gt;
  &lt;li&gt;Our paper is available &lt;a href=&quot;https://arxiv.org/abs/1904.07814&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;It was presented at the 12th Conference on Field and Service Robotic, the slides are available &lt;a href=&quot;../../pdf/Babin2019a.slides.pdf&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;video&quot;&gt;Video&lt;/h1&gt;
&lt;!-- blank line --&gt;
&lt;figure class=&quot;video_container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/t_cBdiPQ1R8&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/figure&gt;
&lt;!-- blank line --&gt;

&lt;h1 id=&quot;publications&quot;&gt;Publications&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot; reversed=&quot;reversed&quot;&gt;&lt;li&gt;&lt;span id=&quot;Babin2019a&quot;&gt;Babin, P., Dandurand, P., Kubelka, V., Giguère, P., &amp;amp; Pomerleau, F. (2019). Large-scale 3D Mapping of Subarctic Forests. In &lt;i&gt;Proceedings of the Conference on Field and Service Robotics (FSR). Springer Tracts in Advanced Robotics&lt;/i&gt;.&lt;/span&gt;


&lt;a href=&quot;/pdf/Babin2019a.pdf&quot;&gt;&lt;i class=&quot;fas fa-file-pdf&quot;&gt;&lt;/i&gt; PDF&lt;/a&gt;


&lt;a href=&quot;/pdf/Babin2019a.slides.pdf&quot;&gt;&lt;i class=&quot;fas fa-file-powerpoint&quot;&gt;&lt;/i&gt; Slides&lt;/a&gt;

&lt;a class=&quot;details&quot; href=&quot;/bibliography/Babin2019a/&quot;&gt; &lt;i class=&quot;fas fa-file-code&quot;&gt;&lt;/i&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Philippe Babin</name></author><category term="project" /><category term="icp" /><category term="mapping" /><category term="subarctic" /><category term="snowmobile" /><category term="forestry" /><category term="lidar" /><summary type="html">The ability to map challenging subarctic environments opens new horizons for robotic deployments in industries such as forestry, surveillance, and open-pit mining. In this paper, we explore possibilities of large-scale lidar mapping in a boreal forest. Computational and sensory requirements with regards to contemporary hardware are considered as well. The lidar mapping is often based on the SLAM technique relying on pose graph optimization, which fuses the Iterative Closest Point (ICP) algorithm, Global Navigation Satellite System (GNSS) positioning, and Inertial Measurement Unit (IMU) measurements. To handle those sensors directly within the ICP minimization process, we propose an alternative approach of embedding external constraints. Furthermore, a novel formulation of a cost function is presented and cast into the problem of handling uncertainties from GNSS and lidar points. To test our approach, we acquired a large-scale dataset in the Foret Montmorency research forest. We report on the technical problems faced during our winter deployments aiming at building 3D maps using our new cost function. Those maps demonstrate both global and local consistency over 4.1km.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22feature%22=%3E%22projects/penalty_icp_feature.jpg%22,%20%22teaser%22=%3E%22projects/penalty_icp_teaser.jpg%22,%20%22thumb%22=%3Enil%7D" /></entry><entry><title type="html">Lambda-Field: A Continuous Counterpart of the Bayesian Occupancy Grid for Risk Assessment</title><link href="https://norlab.ulaval.ca/publications/lambda-field/" rel="alternate" type="text/html" title="Lambda-Field: A Continuous Counterpart of the Bayesian Occupancy Grid for Risk Assessment" /><published>2019-09-02T00:00:00-04:00</published><updated>2019-09-02T00:00:00-04:00</updated><id>https://norlab.ulaval.ca/publications/lambda-field</id><content type="html" xml:base="https://norlab.ulaval.ca/publications/lambda-field/">&lt;p&gt;In a context of autonomous robots, one of the most important tasks is to ensure the safety of the robot and its surrounding.
The risk of navigation is usually said to be the probability of collision.
This notion of risk is not well defined in the literature, especially when dealing with occupancy grids.
The Bayesian occupancy grid is the most used method to deal with complex environments.
However, this is not fitted to compute the risk along a path by its discrete nature.
In this article, we present a new way to store the occupancy of the environment that allows the computation of risk along a given path.
We then define the risk as the force of collision that would occur for a given obstacle.
Using this framework, we are able to generate navigation paths ensuring the safety of the robot.&lt;/p&gt;

&lt;h1 id=&quot;contributions&quot;&gt;Contributions&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;A novel type of map, called Lambda-Field, specially conceived to allow path integrals and thus probabilities of collision;&lt;/li&gt;
  &lt;li&gt;A definition of the risk encountered over a path, specified as the expected force of collision along a path.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;results-in-images&quot;&gt;Results in Images&lt;/h1&gt;

&lt;p&gt;Using the lambda-field, we are able to construct maps where the probability of collision along a path logically arises from the theory.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/publications/lambda-field/maps.png&quot; style=&quot;width: 100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The robot (trajectory in blue) maps an urban-like environment and creates a Bayesian Grid as well as a Lambda-Field.
Although the maps are almost the same, the Lambda-Field tends to better store the unstructured obstacles (bushes in this example).&lt;/p&gt;

&lt;h1 id=&quot;in-video&quot;&gt;In Video&lt;/h1&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/ybj8NWWbzAo&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;p&gt;&lt;span id=&quot;Laconte2019a&quot;&gt;Laconte, J., Debain, C., Chapuis, R., Pomerleau, F., &amp;amp; Aufrere, R. (2019). Lambda-Field: A Continuous Counterpart of the Bayesian Occupancy Grid for Risk Assessment. In &lt;i&gt;Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)&lt;/i&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;links&quot;&gt;Links&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.researchgate.net/publication/331565267_Lambda-Field_A_Continuous_Counterpart_of_the_Bayesian_Occupancy_Grid_for_Risk_Assessment&quot;&gt;Download link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Johann Laconte</name></author><category term="publications" /><category term="traversability" /><category term="lidar" /><category term="mapping" /><summary type="html">In a context of autonomous robots, one of the most important tasks is to ensure the safety of the robot and its surrounding. The risk of navigation is usually said to be the probability of collision. This notion of risk is not well defined in the literature, especially when dealing with occupancy grids. The Bayesian occupancy grid is the most used method to deal with complex environments. However, this is not fitted to compute the risk along a path by its discrete nature. In this article, we present a new way to store the occupancy of the environment that allows the computation of risk along a given path. We then define the risk as the force of collision that would occur for a given obstacle. Using this framework, we are able to generate navigation paths ensuring the safety of the robot.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22teaser%22=%3E%22publications/lambda-field_teaser.png%22,%20%22feature%22=%3E%22publications/lambda-field_feature.png%22,%20%22thumb%22=%3Enil%7D" /></entry><entry><title type="html">Norlab is going to Asia!</title><link href="https://norlab.ulaval.ca/news/iros-fsr19/" rel="alternate" type="text/html" title="Norlab is going to Asia!" /><published>2019-06-24T00:00:00-04:00</published><updated>2019-06-24T00:00:00-04:00</updated><id>https://norlab.ulaval.ca/news/iros-fsr19</id><content type="html" xml:base="https://norlab.ulaval.ca/news/iros-fsr19/">&lt;p&gt;We are going to Tokyo (Japan) and Macau (China) with four accepted publications at the &lt;a href=&quot;http://www.srg.mech.keio.ac.jp/fsr2019/&quot;&gt;2019 International Conference on Field and Service Robotics&lt;/a&gt; and one at the &lt;a href=&quot;https://www.iros2019.org/&quot;&gt;2019 IEEE/RSJ International Conference on Intelligent Robots and Systems&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We will present our latest results on lambda-field maps for continuous and risk-aware path planning &lt;a href=&quot;#Laconte2019a&quot;&gt;(Laconte, Debain, Chapuis, Pomerleau, &amp;amp; Aufrere, 2019)&lt;/a&gt;, 3D mapping in subarctic environments &lt;a href=&quot;#Babin2019a&quot;&gt;(Babin, Dandurand, Kubelka, Giguère, &amp;amp; Pomerleau, 2019)&lt;/a&gt;, estimation of GPS satellite visibility given 3D maps &lt;a href=&quot;#Dandurand2019&quot;&gt;(Dandurand, Babin, Kubelka, Giguère, &amp;amp; Pomerleau, 2019)&lt;/a&gt;, automatic forestry inventory based on 3D maps&lt;a href=&quot;#Tremblay2019&quot;&gt;(Tremblay, Béland, Pomerleau, Gagnon, &amp;amp; Giguère, 2019)&lt;/a&gt;, and multi-session lake-shore monitoring based on lidar &lt;a href=&quot;#Pradalier2019&quot;&gt;(Pradalier, Aravecchia, &amp;amp; Pomerleau, 2019)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;More details, and hopefully videos to come!&lt;/p&gt;

&lt;h1 id=&quot;our-articles&quot;&gt;Our articles&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot; reversed=&quot;reversed&quot;&gt;&lt;li&gt;&lt;span id=&quot;Laconte2019a&quot;&gt;Laconte, J., Debain, C., Chapuis, R., Pomerleau, F., &amp;amp; Aufrere, R. (2019). Lambda-Field: A Continuous Counterpart of the Bayesian Occupancy Grid for Risk Assessment. In &lt;i&gt;Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)&lt;/i&gt;.&lt;/span&gt;



&lt;a class=&quot;details&quot; href=&quot;/bibliography/Laconte2019a/&quot;&gt; &lt;i class=&quot;fas fa-file-code&quot;&gt;&lt;/i&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Dandurand2019&quot;&gt;Dandurand, P., Babin, P., Kubelka, V., Giguère, P., &amp;amp; Pomerleau, F. (2019). Predicting GNSS satellite visibility from dense point clouds. In &lt;i&gt;Proceedings of the Conference on Field and Service Robotics (FSR). Springer Tracts in Advanced Robotics&lt;/i&gt;.&lt;/span&gt;


&lt;a href=&quot;/pdf/Dandurand2019.pdf&quot;&gt;&lt;i class=&quot;fas fa-file-pdf&quot;&gt;&lt;/i&gt; PDF&lt;/a&gt;


&lt;a class=&quot;details&quot; href=&quot;/bibliography/Dandurand2019/&quot;&gt; &lt;i class=&quot;fas fa-file-code&quot;&gt;&lt;/i&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Babin2019a&quot;&gt;Babin, P., Dandurand, P., Kubelka, V., Giguère, P., &amp;amp; Pomerleau, F. (2019). Large-scale 3D Mapping of Subarctic Forests. In &lt;i&gt;Proceedings of the Conference on Field and Service Robotics (FSR). Springer Tracts in Advanced Robotics&lt;/i&gt;.&lt;/span&gt;


&lt;a href=&quot;/pdf/Babin2019a.pdf&quot;&gt;&lt;i class=&quot;fas fa-file-pdf&quot;&gt;&lt;/i&gt; PDF&lt;/a&gt;


&lt;a href=&quot;/pdf/Babin2019a.slides.pdf&quot;&gt;&lt;i class=&quot;fas fa-file-powerpoint&quot;&gt;&lt;/i&gt; Slides&lt;/a&gt;

&lt;a class=&quot;details&quot; href=&quot;/bibliography/Babin2019a/&quot;&gt; &lt;i class=&quot;fas fa-file-code&quot;&gt;&lt;/i&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Tremblay2019&quot;&gt;Tremblay, J.-F., Béland, M., Pomerleau, F., Gagnon, R., &amp;amp; Giguère, P. (2019). Automatic 3D Mapping for Tree Diameter Measurements in Inventory Operations. In &lt;i&gt;Proceedings of the Conference on Field and Service Robotics (FSR). Springer Tracts in Advanced Robotics&lt;/i&gt;.&lt;/span&gt;


&lt;a href=&quot;/pdf/Tremblay2019.pdf&quot;&gt;&lt;i class=&quot;fas fa-file-pdf&quot;&gt;&lt;/i&gt; PDF&lt;/a&gt;


&lt;a href=&quot;/pdf/Tremblay2019.slides.pdf&quot;&gt;&lt;i class=&quot;fas fa-file-powerpoint&quot;&gt;&lt;/i&gt; Slides&lt;/a&gt;

&lt;a class=&quot;details&quot; href=&quot;/bibliography/Tremblay2019/&quot;&gt; &lt;i class=&quot;fas fa-file-code&quot;&gt;&lt;/i&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Pradalier2019&quot;&gt;Pradalier, C., Aravecchia, S., &amp;amp; Pomerleau, F. (2019). Multi-session lake-shore monitoring in visually challenging conditions. In &lt;i&gt;Proceedings of the Conference on Field and Service Robotics (FSR). Springer Tracts in Advanced Robotics&lt;/i&gt;.&lt;/span&gt;



&lt;a class=&quot;details&quot; href=&quot;/bibliography/Pradalier2019/&quot;&gt; &lt;i class=&quot;fas fa-file-code&quot;&gt;&lt;/i&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>François Pomerleau</name><email>francois.pomerleau@ift.ulaval.ca</email></author><category term="conference" /><summary type="html">We are going to Tokyo (Japan) and Macau (China) with four accepted publications at the 2019 International Conference on Field and Service Robotics and one at the 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22teaser%22=%3E%22/news/iros_fsr19/iros_fsr19_teaser.jpg%22%7D" /></entry><entry><title type="html">Lidar Measurement Bias Estimation via Return Waveform Modelling in a Context of 3D Mapping</title><link href="https://norlab.ulaval.ca/publications/lidar-bias/" rel="alternate" type="text/html" title="Lidar Measurement Bias Estimation via Return Waveform Modelling in a Context of 3D Mapping" /><published>2019-05-27T00:00:00-04:00</published><updated>2019-05-27T00:00:00-04:00</updated><id>https://norlab.ulaval.ca/publications/lidar-bias</id><content type="html" xml:base="https://norlab.ulaval.ca/publications/lidar-bias/">&lt;p&gt;In a context of 3D mapping, it is very important to obtain accurate measurements from sensors.
In particular, LIDAR measurements are typically treated as a zero-mean Gaussian distribution.
We show that this assumption leads to predictable localisation drifts, especially when a bias related to measuring obstacles with high incidence angles is not taken into consideration.
Moreover, we present a way to physically understand and model this bias, which generalizes to multiple sensors.
Using an experimental setup, we measured the bias of the Sick LMS-151, Velodyne HDL-32E, and Robosense RS-LiDAR-16 as a function of depth and incidence angle, and showed that the bias can reach 20 cm for high incidence angles.
We then used our model to remove the bias from the measurements, leading to more accurate maps and a reduced localisation drift.&lt;/p&gt;
&lt;h1 id=&quot;contributions&quot;&gt;Contributions&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Physical explanation of a lidar bias caused by the incidence angle of the laser beam on a surface&lt;/li&gt;
  &lt;li&gt;A way to quantify and correct this bias for common LIDARs used in mobile robotics&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;results-in-images&quot;&gt;Results in Images&lt;/h1&gt;

&lt;p&gt;Using our model, we found the following bias for three commonly used LIDARs:
&lt;img src=&quot;/images/publications/lidar-bias/isocurves.jpg&quot; style=&quot;width: 100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We were able to remove the bias from the measurements, leading to more accurate maps.
In blue, the map of a tunnel without taking into account the bias.
In red, the same map taking into account the bias and removing it from the measurements.
&lt;img src=&quot;/images/publications/lidar-bias/tunnels.jpg&quot; style=&quot;width: 100%&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;in-video&quot;&gt;In Video&lt;/h1&gt;

&lt;!--&lt;iframe src=&quot;https://www.youtube.com/watch?v=YE-oL7do2HM&quot; style=&quot;width: 100%&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;--&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/YE-oL7do2HM&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;p&gt;&lt;span id=&quot;Laconte2019&quot;&gt;Laconte, J., Deschênes, S.-P., Labussière, M., &amp;amp; Pomerleau, F. (2019). Lidar Measurement Bias Estimation via Return Waveform Modelling in a Context of 3D Mapping. In &lt;i&gt;Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)&lt;/i&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;links&quot;&gt;Links&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.researchgate.net/publication/328063232_Lidar_Measurement_Bias_Estimation_via_Return_Waveform_Modelling_in_a_Context_of_3D_Mapping&quot;&gt;Download link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Johann Laconte</name></author><category term="publications" /><category term="ICP" /><category term="bias estimation" /><category term="sensor error modelling" /><category term="lidar" /><category term="mapping" /><summary type="html">In a context of 3D mapping, it is very important to obtain accurate measurements from sensors. In particular, LIDAR measurements are typically treated as a zero-mean Gaussian distribution. We show that this assumption leads to predictable localisation drifts, especially when a bias related to measuring obstacles with high incidence angles is not taken into consideration. Moreover, we present a way to physically understand and model this bias, which generalizes to multiple sensors. Using an experimental setup, we measured the bias of the Sick LMS-151, Velodyne HDL-32E, and Robosense RS-LiDAR-16 as a function of depth and incidence angle, and showed that the bias can reach 20 cm for high incidence angles. We then used our model to remove the bias from the measurements, leading to more accurate maps and a reduced localisation drift. Contributions</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22feature%22=%3E%22publications/lidar-bias_feature.jpg%22,%20%22teaser%22=%3E%22publications/lidar-bias_teaser.jpg%22,%20%22thumb%22=%3Enil%7D" /></entry><entry><title type="html">Predicting GNSS satellite visibility from dense point clouds</title><link href="https://norlab.ulaval.ca/publications/satellite-visibility/" rel="alternate" type="text/html" title="Predicting GNSS satellite visibility from dense point clouds" /><published>2019-05-09T00:00:00-04:00</published><updated>2019-05-09T00:00:00-04:00</updated><id>https://norlab.ulaval.ca/publications/satellite-visibility</id><content type="html" xml:base="https://norlab.ulaval.ca/publications/satellite-visibility/">&lt;p&gt;To help future mobile agents plan their movement in harsh environments,a predictive model has been designed to determine what areas would be favorable for Global Navigation Satellite System (GNSS) positioning. The model is able to predict the number of viable satellites for a GNSS receiver, based on a 3D point cloud map and a satellite constellation. Both occlusion and absorption effects of the environment are considered. A rugged mobile platform was designed to collect data in order to generate the point cloud maps. It was deployed during the Canadian winter known for large amounts of snow and extremely low temperatures. The test environments include a highly dense boreal forest and a university campus with high buildings. The experiment results indicate that the model performs well in both structured and unstructured environments.&lt;/p&gt;

&lt;h1 id=&quot;quick-facts&quot;&gt;Quick facts:&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;This method has been tested in diverse environments such has a highly dense boreal forest and a university campus with high buildings&lt;/li&gt;
  &lt;li&gt;This method is able to differentiate between forest and buildings&lt;/li&gt;
  &lt;li&gt;The hardware used in the paper has been proven to work in sub-arctic environment&lt;/li&gt;
  &lt;li&gt;Our preprint is available &lt;a href=&quot;https://arxiv.org/abs/1904.07837&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;video&quot;&gt;Video&lt;/h1&gt;
&lt;!-- blank line --&gt;
&lt;figure class=&quot;video_container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/5LdxV1-9rEE&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/figure&gt;
&lt;!-- blank line --&gt;

&lt;h1 id=&quot;publications&quot;&gt;Publications&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot; reversed=&quot;reversed&quot;&gt;&lt;li&gt;&lt;span id=&quot;Dandurand2019&quot;&gt;Dandurand, P., Babin, P., Kubelka, V., Giguère, P., &amp;amp; Pomerleau, F. (2019). Predicting GNSS satellite visibility from dense point clouds. In &lt;i&gt;Proceedings of the Conference on Field and Service Robotics (FSR). Springer Tracts in Advanced Robotics&lt;/i&gt;.&lt;/span&gt;


&lt;a href=&quot;/pdf/Dandurand2019.pdf&quot;&gt;&lt;i class=&quot;fas fa-file-pdf&quot;&gt;&lt;/i&gt; PDF&lt;/a&gt;


&lt;a class=&quot;details&quot; href=&quot;/bibliography/Dandurand2019/&quot;&gt; &lt;i class=&quot;fas fa-file-code&quot;&gt;&lt;/i&gt; Bibtex source&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Philippe Dandurand</name></author><category term="project" /><category term="GNSS" /><category term="GPS" /><category term="lidar" /><category term="RTK" /><category term="DGNSS" /><category term="winter" /><category term="mapping" /><category term="uncertainty" /><summary type="html">To help future mobile agents plan their movement in harsh environments,a predictive model has been designed to determine what areas would be favorable for Global Navigation Satellite System (GNSS) positioning. The model is able to predict the number of viable satellites for a GNSS receiver, based on a 3D point cloud map and a satellite constellation. Both occlusion and absorption effects of the environment are considered. A rugged mobile platform was designed to collect data in order to generate the point cloud maps. It was deployed during the Canadian winter known for large amounts of snow and extremely low temperatures. The test environments include a highly dense boreal forest and a university campus with high buildings. The experiment results indicate that the model performs well in both structured and unstructured environments.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://norlab.ulaval.ca/%7B%22feature%22=%3E%22projects/sat_vis_feature.jpg%22,%20%22teaser%22=%3E%22projects/sat_vis_teaser.jpg%22,%20%22thumb%22=%3Enil%7D" /></entry></feed>